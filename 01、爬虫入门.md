# 1、概述

爬虫，应该称为网络爬虫，也叫网页蜘蛛、网络机器人、网络蚂蚁等。搜索引擎，就是网络爬虫的应用者。

为什么到了今天，反而这个词汇被频繁的提起呢？有搜索引擎不就够了吗？

实际上，大数据时代的到了，所有的企业都希望通过海量数据发现其中的价值。

所以，需要爬取对特定网站、特定类别的数据，而搜索引擎不能提供这样的功能，因此，需要自己开发爬虫来解决。

## 1.1 爬虫分类

### 1.1.1 通用爬虫

常见的就是搜索引擎（网站的自动采集、网站内容的自动更新），无差别的收集数据、存储，提取关键字，构建索引库，给用户提供搜索接口，所以爬虫只是一个开始。

__爬取一般流程：__ 

1.  初始一批 URL，将这些 URL 放到待爬取队列；

2.  从队列取出这些 URL，通过 DNS 解析 IP，对 IP 对应的站点下载 HTML 页面，保存到本地服务器中，爬取完的 URL 放到已爬取队列；

3.  分析这些网页内容，找出网页里面的其他关心的 URL 链接，继续执行第 2 步，直到爬取条件结束。

__搜索引擎如何获取一个新网站的 URL？__ 

*   新网站主动提交给搜索引擎

*   通过其它网站页面中设置的外链

*   搜索引擎和 DNS 服务商合作，获取最新收录的网站

### 1.1.2 聚焦爬虫

有针对的编写特定领域数据（体育板块、娱乐板块）的爬取程序，针对某些类别数据采集的爬虫，是面向主题的爬虫。

## 1.2 Robots 协议

指定一个 robot.txt 文件，告诉爬虫引擎什么可以爬取，实例（淘宝 robot.txt）：

```
User-agent: Baiduspider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /product/
Disallow: /
允许Baiduspider爬寻article、oshtml、wenzhang整个目录，禁止爬寻product目录下面的目录。其它禁止访问。

User-Agent: Googlebot
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Googlebot爬寻article、oshtml、product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-agent: Bingbot
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Bingbot爬寻article、oshtml、product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-Agent: 360Spider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /
允许360Spider爬寻article、oshtml、wenzhang整个目录。其它禁止访问。

User-Agent: Yisouspider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /
允许Yisouspider 爬寻article、oshtml、wenzhang整个目录。其它禁止访问。

User-Agent: Sogouspider
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /wenzhang
Disallow: /
允许Sogouspider爬寻article、oshtml、product、wenzhang整个目录。其它禁止访问。

User-Agent: Yahoo! Slurp
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Yahoo! Slurp爬寻product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-Agent: *
Disallow: /
禁止所有搜索引擎访问网站的任何部分
```

- 其它爬虫，不允许爬取
- 这是一个君子协定，“爬亦有道”

这个协议为了让搜素引擎更有效率搜索自己内容，提供了如 Sitemap 这样的文件。

这个文件禁止爬取的往往又是可能我们感兴趣的内容，它反而泄露了这些地址。

## 1.3 网页源码分析



# 2、 HTTP请求和响应处理

其实爬取网页就是通过HTTP协议访问网页，不过通过浏览器访问往往是人的行为，把这种行为变成使用程序来访问。

## 2.1 urllib包

urllib 是标准库，它是一个工具包模块，包含下面模块来处理 url：

*   urllib.request 用于打开和读写 url

*   urllib.error 包含了由 urllib.request 引起的异常

*   urllib.parse 用于解析 url

*   urllib.robotparser 分析 robots.txt 文件

Python2 中提供了 urllib 和 urllib2。urllib 提供较为底层的接口，urllib2 对 urllib 进行了进一步封装。Python3 中将 urllib 合并到了 urllib2 中，并只提供了标准库 urllib 包。

### 2.1.1 urllib.request模块

模块定义了在基本和摘要式身份验证、重定向、cookies 等应用中打开 url（主要是HTTP）的函数和类。

**urlopen方法：**

```python
# 语法
urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):

# 参数:
  url    : 链接地址字符串或request对象
  data   : 提交的数据，如果为None发起GET请求，否则发起POST请求。返回http.client.HTTPResponse类的响应对象这是一个类文件对象。
  timeout: 超时时间
  ca*    : 证书
```

```python
from urllib.request import urlopen
from http.client import HTTPResponse

# 打开一个url返回一个响应对象(类文件对象)
# 下面的链接访问后会有跳转
url = "http://www.bing.com"        # 301/302自动跳转
response = urlopen(url, timeout=5) # GET方法
print(response.closed)    # False
with response:
    print(1, type(response))       # http.client.HTTPResponse类文件对象
    print(2, response.status, response.reason)    # 状态
    print(3, response.geturl())    # 返回真正的URL
    print(4, response.info())      # headers
    print(5, response.read())      # 读取返回的内容
    print(6, response._method)     # GET方法

print(response.closed)     # True
```

上例，通过 urllib.request.urlopen 方法，发起一个HTTP的GET请求，WEB服务器返回了网页内容。响应的数据被封装到类文件对象中，可以通过 read 方法、readline 方法、readlines 方法获取数据，status 和 reason 属性表示返回的状态码，info 方法返回头信息，等等。

**User-Agent 问题：**

上例的代码非常精简，即可以获得网站的响应数据。urlopen 方法只能传递url 和 data 这样的数据，不能构造HTTP的请求。例如 useragent。

源码中构造的 useragent 如下：

```python
from urllib.request import OpenerDirector    # Ctrl+点击: OpenerDirector，查看源码
class OpenerDirector:
    def __init__(self):
        client_version = "Python-urllib/%s" % __version__
        self.addheaders = [('User-agent', client_version)]
```

当前显示为：`Python-urllib/3.7`

有些网站是反爬虫的，所以要把爬虫伪装成浏览器。随便打开一个浏览器，复制浏览器的UA值用来伪装。

**User-agent大全：**

```python
'''
# Opera
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60
Opera/8.0 (Windows NT 5.1; U; en)
Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50
 
# Firefox
Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0
Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10
 
# Safari
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2
 
# chrome
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
 
# 360
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36
Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko
 
# 淘宝浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11
 
# 猎豹浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER) 
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)"

# QQ浏览器
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)

# sogou浏览器
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0
Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)

# maxthon浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36

# UC浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36
'''
```

**Request类：**

`Request(url, data=None, headers={})`

初始化方法，构造一个请求对象。可添加一个 header 的字典。data 参数决定是GET还是POST请求。

`add_header(key, val)` 为 header 中增加一个键值对。

```python
from urllib.request import urlopen, Request
import random

# 打开一个url返回一个Request请求对象
url = "http://www.bing.com/"    # 注意尾部的"/"一定要有

ua_list = [
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;",
    "Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)"

]

req = Request(url)
req.add_header('User-agent', random.choice(ua_list))    # pick one: 随机选择一个
print(type(req))

response = urlopen(req, timeout=5)    # url或request对象
print(type(response))

with response:
    print(1, response.status, response.getcode(), response.reason)    # 状态，getcode本质就是返回status
    print(2, response.geturl())    # 返回数据的url，如果重定向，这个url和原来url不一样
    # 例如原始url是http://www.bing.com/，返回http://cn.bing.com/
    print(3, response.info())      # 返回响应头headers
    print(4, response.read())      # 读取返回的内容

print(5, req.get_header('User-agent'))
print(6, 'user-agent'.capitalize())
```

### 2.1.2 urllib.parse模块

该模块可以完成对 url 的编解码

以下代码，进行编码：

```python
from urllib import parse
u = parse.urlencode('http://www.mageedu.com/python')

# 运行结果如下
Traceback (most recent call last):
  File "E:/PycharmProjects/基础/0、demo/demo3.py", line 2, in <module>
    u = parse.urlencode('http://www.mageedu.com/python')
  File "E:\课程软件安装\Python\Python37\lib\urllib\parse.py", line 909, in urlencode
    "or mapping object").with_traceback(tb)
  File "E:\课程软件安装\Python\Python37\lib\urllib\parse.py", line 901, in urlencode
    raise TypeError
TypeError: not a valid non-string sequence or mapping object
```

urlencode 函数第一参数要求是一个字典或者二元组序列。

```python
from urllib import parse

# 实例一：
u = parse.urlencode({
    "id":1,
    "name":"tom"
})
print(u)    # id=1&name=tom


# 实例二：
"""
url = "http://www.bing.com/?id=1&name=tom" ——> GET
url = "http://www.bing.com/" ——> POST
body = "id=1&name=tom"
"""
u = parse.urlencode({
    "id":1,
    "name":"tom",
    "url":"http://www.bing.com/?id=1&name=tom"
})
print(u)    # id=1&name=tom&url=http%3A%2F%2Fwww.bing.com%2F%3Fid%3D1%26name%3Dtom


# 实例三：
u = parse.urlencode({
    "id":1,
    "name":"张三",
    "url":"http://www.bing.com/?id=1&name=张三"
})
print(u)    # id=1&name=%E5%BC%A0%E4%B8%89&url=http%3A%2F%2Fwww.bing.com%2F%3Fid%3D1%26name%3D%E5%BC%A0%E4%B8%89
```

从运行结果来看冒号、斜杠、&、等号、问号等符号全部被编码了，%之后实际上是单字节十六进制表示的值。

一般来说 url 中的地址部分，一般不需要使用中文路径，但是参数部分，不管GET还是POST方法，提交的数据中，可能有斜杆、等号、问号等符号，这样这些字符表示数据，不表示元字符。如果直接发给服务器端，就会导致接收方无法判断谁是元字符，谁是数据了。为了安全，一般会将数据部分**的**字符做 url 编码，这样就不会有歧义了。后来可以传送中文，同样会做编码，一般先按照字符集的 encoding 要求转换成字节序列，每一个字节对应的十六进制字符串前加上百分号即可。

```python
# 实验四：
# 网页使用utf-8编码
# https://www.baidu.com/s?wd=中
# 上面的url编码后为：https://www.baidu.com/s?wd=%E4%B8%AD

u = parse.urlencode({'wd':'中'})    # 编码 ——> wd=%E4%B8%AD
url = "https://www.baidu.com/s?{}".format(u)
print(url)    # https://www.baidu.com/s?wd=%E4%B8%AD

print('中'.encode('utf-8'))    # b'\xe4\xb8\xad'
print(parse.unquote(u))        # 解码 ——> wd=中
print(parse.unquote(url))      # https://www.baidu.com/s?wd=中
```

## 2.2 提交方法method

最常用的HTTP交互数据的方法是GET、POST。

GET方法，数据是通过URL传递的，也就是说数据是在HTTP报文的 header 部分。

POST方法，数据是放在HTTP报文的 body 部分提交的。

数据都是键值对形式，多个参数之间使用&符号连接。例如 a=1\&b=abc

### 2.2.1 GET方法

连接必应搜索引擎官网，获取一个搜索的URL：[https://cn.bing.com/search?q=腾讯](https://cn.bing.com/search?q=腾讯 "https://cn.bing.com/search?q=腾讯")

需求：请写程序完成对关键字的 bing 搜索，将返回的结果保存到一个网页文件。

```python
from urllib.request import Request, urlopen
from urllib import parse
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

base_url = 'http://cn.bing.com/search'
d = {'q': '马哥教育'}

u = parse.urlencode(d)
url = "{}?{}".format(base_url, u)
print(url)    # http://cn.bing.com/search?q=%E9%A9%AC%E5%93%A5%E6%95%99%E8%82%B2
print(parse.unquote(url))    # http://cn.bing.com/search?q=马哥教育

# 伪装
ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
req = Request(url, headers={
    'User-agent': ua
})

with urlopen(req) as res:
    with open('F:/bing.html', 'wb+') as f:    # 生成：F:/bing.html
        f.write(res.read())
        f.flush()
```

### 2.2.2 POST方法

[http://httpbin.org/](http://httpbin.org/ "http://httpbin.org/") 测试网站

```python
from urllib.request import Request, urlopen
from urllib.parse import urlencode

req = Request('http://httpbin.org/post')
req.add_header(
    'User-agent',
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
)

data = urlencode({'name': '张三,@=/&*', 'age': '6'})
print(data)

# POST方法,From提交数据,不做url编码会有风险
with urlopen(req, data=data.encode()) as res:
    print(res.read())


# 执行结果i
name=%E5%BC%A0%E4%B8%89%2C%40%3D%2F%26%2A&age=6
b'{\n  "args": {}, \n  "data": "", \n  "files": {}, \n  "form": {\n    "age": "6", \n    "name": "\\u5f20\\u4e09,@=/&*"\n  }, \n  "headers": {\n    "Accept-Encoding": "identity", \n    "Content-Length": "47", \n    "Content-Type": "application/x-www-form-urlencoded", \n    "Host": "httpbin.org", \n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36", \n    "X-Amzn-Trace-Id": "Root=1-60606a92-19ed61e913f20e5c659135f4"\n  }, \n  "json": null, \n  "origin": "39.182.24.157", \n  "url": "http://httpbin.org/post"\n}\n'
```

### 2.2.3 处理JSON数据

```python
from urllib.request import Request, urlopen
from urllib.parse import urlencode
import simplejson

req = Request('http://httpbin.org/post')
req.add_header(
    'User-agent',
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
)

data = urlencode({'name': '张三,@=/&*', 'age': '6'})
print(data)

# POST方法,From提交数据,不做url编码会有风险
with urlopen(req, data=data.encode()) as res:
    test = res.read()
    d = simplejson.loads(test)
    print(type(d), d)


# 执行结果
name=%E5%BC%A0%E4%B8%89%2C%40%3D%2F%26%2A&age=6
<class 'dict'> {'args': {}, 'data': '', 'files': {}, 'form': {'age': '6', 'name': '张三,@=/&*'}, 'headers': {'Accept-Encoding': 'identity', 'Content-Length': '47', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36', 'X-Amzn-Trace-Id': 'Root=1-60606b5b-65a908d0439cb0db0dc8ab27'}, 'json': None, 'origin': '39.182.24.157', 'url': 'http://httpbin.org/post'}
```

热门电影爬取：

*   查看豆瓣电影：[https://movie.douban.com/](https://movie.douban.com/ "https://movie.douban.com/")，看到“最近热门电影”的热门并查看源码；

![](<image/截屏2021-07-25 下午2.34.22_Gf6cvMDEpv.png>)

*   通过源码分析，我们知道这部分内容，是通过 AJAX 从后台拿到的 Json 数据。直接查看网页源代码并不能找到；

*   源码 `--->` Network `--->` XHR `--->` Preview，右击复制链接地址；

![](<image/截屏2021-07-25 下午2.37.23_iEQhn4vk95.png>)

```python
# URL：
https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start=0

type 数据类型，movie 是电影
tag 标签热门，表示热门电影（%E7%83%AD%E9%97%A8）
page_limit 表示返回数据的总数
page_start 表示数据开始偏移位置

```

*   服务器返回的 Json 数据爬取。

```python
from urllib.parse import urlencode
from urllib.request import Request, urlopen
import simplejson
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
url = 'https://movie.douban.com/j/search_subjects'

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

req = Request("{}?{}".format(url, urlencode(d)), headers={'User-agent': ua})

# 有可能失败，可以进行异常处理
with urlopen(req) as res:
    subjects = simplejson.loads(res.read())
    print(len(subjects['subjects']))
    print(subjects)


# 执行结果
20
{'subjects': [{'episodes_info': '', 'rate': '8.9', 'cover_x': 2025, 'title': '扎克·施奈德版正义联盟', 'url': 'https://movie.douban.com/subject/35076714/', 'playable': False, 'cover': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2634360594.jpg', 'id': '35076714', 'cover_y': 3000, 'is_new': False}, {'episodes_info': '', 'rate': '5.7', 'cover_x': 3571, 'title': '侍神令', 'url': 'https://movie.douban.com/subject/26935283/', 'playable': True, 'cover': 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2629260713.jpg', 'id': '26935283', 'cover_y': 4990, 'is_new': False}, {'episodes_info': '', 'rate': '7.2', 'cover_x': 826, 'title': '飙速宅男', 'url': 'https://movie.douban.com/subject/26935250/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2611588399.jpg', 'id': '26935250', 'cover_y': 1170, 'is_new': True}, {'episodes_info': '', 'rate': '7.8', 'cover_x': 1434, 'title': '倒霉性爱，发狂黄片', 'url': 'https://movie.douban.com/subject/35242942/', 'playable': False, 'cover': 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2633509953.jpg', 'id': '35242942', 'cover_y': 2048, 'is_new': True}, {'episodes_info': '', 'rate': '6.7', 'cover_x': 3957, 'title': '双层肉排', 'url': 'https://movie.douban.com/subject/35145068/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2633977758.jpg', 'id': '35145068', 'cover_y': 5669, 'is_new': True}, {'episodes_info': '', 'rate': '8.3', 'cover_x': 2700, 'title': '无依之地', 'url': 'https://movie.douban.com/subject/30458949/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2630453887.jpg', 'id': '30458949', 'cover_y': 4000, 'is_new': False}, {'episodes_info': '', 'rate': '7.3', 'cover_x': 4950, 'title': '寻龙传说', 'url': 'https://movie.douban.com/subject/34804147/', 'playable': False, 'cover': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2633531206.jpg', 'id': '34804147', 'cover_y': 6900, 'is_new': False}, {'episodes_info': '', 'rate': '8.5', 'cover_x': 2025, 'title': '亲爱的同志', 'url': 'https://movie.douban.com/subject/34960094/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2630627669.jpg', 'id': '34960094', 'cover_y': 3000, 'is_new': False}, {'episodes_info': '', 'rate': '7.0', 'cover_x': 810, 'title': '沿路而下', 'url': 'https://movie.douban.com/subject/30456637/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2633150167.jpg', 'id': '30456637', 'cover_y': 1200, 'is_new': False}, {'episodes_info': '', 'rate': '7.9', 'cover_x': 992, 'title': '同学麦娜丝', 'url': 'https://movie.douban.com/subject/34902639/', 'playable': False, 'cover': 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2623673142.jpg', 'id': '34902639', 'cover_y': 1418, 'is_new': False}, {'episodes_info': '', 'rate': '8.2', 'cover_x': 5000, 'title': '打开心世界', 'url': 'https://movie.douban.com/subject/30454679/', 'playable': False, 'cover': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2632936000.jpg', 'id': '30454679', 'cover_y': 7071, 'is_new': False}, {'episodes_info': '', 'rate': '6.0', 'cover_x': 1656, 'title': '发财日记', 'url': 'https://movie.douban.com/subject/27594653/', 'playable': True, 'cover': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2632091530.jpg', 'id': '27594653', 'cover_y': 2500, 'is_new': False}, {'episodes_info': '', 'rate': '6.7', 'cover_x': 5000, 'title': '许愿神龙', 'url': 'https://movie.douban.com/subject/27662747/', 'playable': True, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2626260539.jpg', 'id': '27662747', 'cover_y': 7053, 'is_new': False}, {'episodes_info': '', 'rate': '8.0', 'cover_x': 2067, 'title': '孤味', 'url': 'https://movie.douban.com/subject/34805873/', 'playable': False, 'cover': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2623646280.jpg', 'id': '34805873', 'cover_y': 2953, 'is_new': False}, {'episodes_info': '', 'rate': '8.6', 'cover_x': 1500, 'title': '芝加哥七君子审判', 'url': 'https://movie.douban.com/subject/2609258/', 'playable': False, 'cover': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2620161520.jpg', 'id': '2609258', 'cover_y': 2222, 'is_new': False}, {'episodes_info': '', 'rate': '8.3', 'cover_x': 1200, 'title': '浅田家！', 'url': 'https://movie.douban.com/subject/26958479/', 'playable': False, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2614188527.jpg', 'id': '26958479', 'cover_y': 1698, 'is_new': True}, {'episodes_info': '', 'rate': '7.5', 'cover_x': 3264, 'title': '送你一朵小红花', 'url': 'https://movie.douban.com/subject/35096844/', 'playable': True, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2618247457.jpg', 'id': '35096844', 'cover_y': 4929, 'is_new': False}, {'episodes_info': '', 'rate': '7.9', 'cover_x': 6198, 'title': '吉祥如意', 'url': 'https://movie.douban.com/subject/35068230/', 'playable': True, 'cover': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2632185061.jpg', 'id': '35068230', 'cover_y': 8685, 'is_new': False}, {'episodes_info': '', 'rate': '6.2', 'cover_x': 2764, 'title': '神奇女侠1984', 'url': 'https://movie.douban.com/subject/27073752/', 'playable': True, 'cover': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2626959989.jpg', 'id': '27073752', 'cover_y': 4096, 'is_new': False}, {'episodes_info': '', 'rate': '8.8', 'cover_x': 6611, 'title': '心灵奇旅', 'url': 'https://movie.douban.com/subject/24733428/', 'playable': True, 'cover': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626308994.jpg', 'id': '24733428', 'cover_y': 9435, 'is_new': False}]}
```

## 2.3 HTTPS证书忽略

HTTPS使用SSL安全套接层协议，在传输层对网络数据进行加密。HTTPS使用的时候需要证书，而证书需要CA认证。

CA(Certificate Authority) 是数字证书认证中心的简称，是指发放、管理、废除数字证书的机构。

CA是受信任的第三方，有CA签发的证书具有可信性。如果用户由于信任了CA签发的证书导致的损失，可以追究CA的法律责任。

CA是层级结构，下级CA信任上级CA，且有上级CA颁发给下级CA证书并认证。

—些网站，例如淘宝，使用HTTPS加密数据更加安全。

```python
from urllib.request import Request, urlopen

# req = Request('http://www.12306.cn/mormhweb/')  # 可以访问
# req = Request('https://www.baidu.com/')         # 可以访问
req = Request('https://www.12306.cn/mormhweb/')   # 报SSL认证异常

req.add_header(
    'User-agent',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'
)

with urlopen(req) as res:
    print(res._method)
    print(res.read())
```

![](<image/截屏2021-07-25 下午3.05.44_MLps5TZboB.png>)

通过HTTPS访问12306的时候，失败的原因在于12306的证书未通过CA认证，它是自己生成的证书，不可信。而其他网站访问，如 [https://www.baidu.com/](https://www.baidu.com/ "https://www.baidu.com/") 并没有提示的原因在于，它的证书的发行者受信任，且早就存储在当前系统中。

能否像浏览器一样，忽略证书不安全信息呢？

```python
from urllib.request import Request, urlopen
import ssl    # 导入ssl模块

# req = Request('http://www.12306.cn/mormhweb/')  # 可以访问
# req = Request('https://www.baidu.com/')         # 可以访问
req = Request('https://www.12306.cn/mormhweb/')   # 报SSL认证异常

req.add_header(
    'User-agent',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'
)

# 忽略不信任的证书: 可以忽略不校验的上下文
context = ssl._create_unverified_context()

res = urlopen(req, context=context)    # 上下文
with res:
    print(res._method)
    print(res.geturl())
    print(res.read().decode())
```

## 2.4 urllib3库

官网网站：[https://urllib3.readthedocs.io/en/latest/](https://urllib3.readthedocs.io/en/latest/ "https://urllib3.readthedocs.io/en/latest/")

标准库 urllib 缺少了一些关键的功能，非标准的第三方库 urllib3 提供了，比如说连接池管理。

安装：`pip install urllib3`

```python
from urllib.parse import urlencode
import urllib3
from urllib3.response import HTTPResponse

url = 'https://movie.douban.com/j/search_subjects'
ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

with urllib3.PoolManager() as http:
    responce = http.request('GET', '{}?{}'.format(url, urlencode(d)), headers={'User-agent': ua})
    print(type(responce))    # <class 'urllib3.response.HTTPResponse'>
    # responce: HTTPResponse = HTTPResponse()
    # responce.
    print(responce.status, responce.reason)
    print(responce.data)
    print(responce.headers)
```

***

## 2.5 request库 \*\*

requests 使用了 urllib3，但是API更加友好，推荐使用。

官方文档：[https://docs.python-requests.org/zh\_CN/latest/](https://docs.python-requests.org/zh_CN/latest/ "https://docs.python-requests.org/zh_CN/latest/")

### 2.5.1 response常用属性

```python
from urllib.parse import urlencode

import requests

jurl = 'https://movie.douban.com/j/search_subjects'
ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

url = '{}?{}'.format(jurl, urlencode(d))

response = requests.request('GET', url, headers={'User-agent': ua})

with response:
    print(1, response.text)               # text
    print(2, response.content)            # bytes
    print(2, response.status_code)        # status
    print(3, response.url)                # url
    print(4, response.headers)            # 响应头
    print(5, response.request.headers)    # 响应对应的请求头
    print(6, response.cookies)            # 响应的cookie（经过了set-cookie动作）
    print(7, response.request._cookies)   # 响应对应请求的cookie

```

**注意：**

*   response.text返回字符串类型，默认“iso-8859-1”编码，服务器不指定的话是根据网页的响应来猜测编码。

*   response.content  返回的是 bytes 型的二进制数据，返回字节类型。

### 2.5.2 User-Agent

同样的和urllib.request模块一样，我们可以使用User-Agent模拟浏览器，欺骗服务器，获取和浏览器一致的内容。

```python
import requests

url = "https://www.baidu.com/"

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
}

response = requests.get(url, headers=headers)

data = response.content

with open("baidu.html", "wb") as f:
    f.write(data)
```

我们可以对比加User-Agent用户代理之前爬取的网页数据和加了User\_Agent用户代理之后爬取的网页数据，发现加了User-Agent之后爬取的数据明显变多；

我们可以写一个User-Agent的列表，每次随机选出一个User-Agent来使用（random.choice）；

fake-useragent库，可以用来伪装请求头（User-Agent），但是不太稳定。

### 2.5.3 url参数

在url地址中， 很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除。如何确定那些请求参数有用或者没用，可以进行参数删除尝试！

原始url：[https://www.baidu.com/s?ie=utf-8\&f=8\&rsv\_bp=1\&rsv\_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv\_pq=dc94bf5400990e93\&rsv\_t=fc46%2BSMZiGcjWe%2FVkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i%2BQvFXstto\&rqlang=cn\&rsv\_enter=1\&rsv\_dl=tb\&rsv\_sug3=9\&rsv\_sug1=8\&rsv\_sug7=101\&rsv\_sug2=0\&rsv\_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv\_sug4=3061\&rsv\_sug=1](https://www.baidu.com/s?ie=utf-8\&f=8\&rsv_bp=1\&rsv_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv_pq=dc94bf5400990e93\&rsv_t=fc46+SMZiGcjWe/VkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i+QvFXstto\&rqlang=cn\&rsv_enter=1\&rsv_dl=tb\&rsv_sug3=9\&rsv_sug1=8\&rsv_sug7=101\&rsv_sug2=0\&rsv_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv_sug4=3061\&rsv_sug=1 "https://www.baidu.com/s?ie=utf-8\&f=8\&rsv_bp=1\&rsv_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv_pq=dc94bf5400990e93\&rsv_t=fc46%2BSMZiGcjWe%2FVkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i%2BQvFXstto\&rqlang=cn\&rsv_enter=1\&rsv_dl=tb\&rsv_sug3=9\&rsv_sug1=8\&rsv_sug7=101\&rsv_sug2=0\&rsv_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv_sug4=3061\&rsv_sug=1")

简单url：[https://www.baidu.com/s?\&wd=背景](https://www.baidu.com/s?\&wd=背景 "https://www.baidu.com/s?\&wd=背景")

**url转义：** urllib.parse模块中的quote/unquote方法

*   quote() : 将明文转为密文

*   unquote() : 将密文转为明文

```python
from urllib.parse import quote, unquote
print(quote("背景"))           # %E8%83%8C%E6%99%AF
print(unquote(quote("背景")))  # 背景
```

**参数请求的形式：**

*   拼接url地址

    `url = "link?key1=value1&key2=value2"`

    `requests.get(url)`

*   字典参数

    `url = "link?"`

    `params = {key1: value1, key2: value2}`

    `requests.get(url, params=params)`

**实例：** 发送带参数的请求

```python
import requests

url = "https://www.baidu.com/s"
val = input("请输入你要查询的页面: ")

params = {
    "wd": val
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
}

response = requests.get(url, headers=headers, params=params)

data = response.content

with open("{}.html".format(val), "wb") as f:
    f.write(data)
```

### 2.5.4 cookie和session

HTTP协议本身是无状态的。什么是无状态呢，即服务器无法判断用户身份。Cookie实际上是一小段的文本信息（key-value格式）。客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。

Session是另一种记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。

如果说Cookie机制是通过检查客户身上的“通行证”来确定客户身份的话，那么Session机制就是通过检查服务器上的“客户明细表”来确认客户身份。Session相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。

#### 2.5.4.1 cookie的使用

*   带上cookie的好处

    能够访问登录后的页面

    能够实现部分反反爬

*   带上cookie的坏处

    一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫

**cookie使用的形式：**

*   参数接收

    `cookies = {"Cookie": "******"}`

    `requests.get(url, headers=headers, cookies=cookies)`

*   记录在headers中传入

    `headers = {"User-Agent": "", "Cookie": ""}`

    `requests.get(url, headers=headers)`

**实例：**

```python
import requests

# 登陆状态的推酷网页
url = "https://www.tuicool.com/a/?i=n"

headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"}
cookies = {"Cookie": "UM_distinctid=17ae2b71b4e19-0c179840ccffbb-35637203-13c680-17ae2b71b4fac6; CNZZDATA5541078=cnzz_eid%3D1507646347-1627296276-%26ntime%3D1627296276; _tuicool_session=bk9PWDF2VFNqK1ExWmx6engzS0dOeG5IU0FyZkw5SGtFM0xIK2NXaUJrMWgyeEkxeStycEZrYzhSRC9oUzRqRURYa1JnTlFsK2JEZUhuOUFJcWNmZFlmSytIaTVYZmJ1OWk4SHg4b21KdkZoUTFFKzc3a1psV0VENDJFWnh5SDNHbXkyNitNSVpaUmxPam0wbVRtOXppalhWTDlkUTJHZWt4ZFVoSjg1TFdqdzMwT3Yvam85dkN4NHJ0NExFWG15QllPYlJTZ0s1cmlNS1pmVHJ6a2lwUUpaVDRkbmlPeVRxbTFSRU9YeTRVYW5EZXhVTCtYalBPMU5kQ0pQclY2UmowenRISGdXWFJKc2pBRDU5cFd0MWFIM0h0R2JMMG1pdmFFcUUyZEZiZTVnRHlqREc4NFpyenBObnRkR1d1VDZUbTF6V0xoeUU0eW1hS09hM1Z0OTFMMk90RVk5S0tpRVRvN3lxb2dKenNjPS0tRTFGQjRkUDlIcERqZHhzS0VMOStHZz09--c64448e8a7bb4d5ee3889e7c47cb512093accc1b"}

response = requests.get(url, headers=headers, cookies=cookies)

data = response.content

with open("tuicool.html", "wb") as f:
    f.write(data)

```

```python
import requests

# 登陆状态的推酷网页
url = "https://www.tuicool.com/a/?i=n"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "Cookie": "UM_distinctid=17ae2b71b4e19-0c179840ccffbb-35637203-13c680-17ae2b71b4fac6; CNZZDATA5541078=cnzz_eid%3D1507646347-1627296276-%26ntime%3D1627296276; _tuicool_session=bk9PWDF2VFNqK1ExWmx6engzS0dOeG5IU0FyZkw5SGtFM0xIK2NXaUJrMWgyeEkxeStycEZrYzhSRC9oUzRqRURYa1JnTlFsK2JEZUhuOUFJcWNmZFlmSytIaTVYZmJ1OWk4SHg4b21KdkZoUTFFKzc3a1psV0VENDJFWnh5SDNHbXkyNitNSVpaUmxPam0wbVRtOXppalhWTDlkUTJHZWt4ZFVoSjg1TFdqdzMwT3Yvam85dkN4NHJ0NExFWG15QllPYlJTZ0s1cmlNS1pmVHJ6a2lwUUpaVDRkbmlPeVRxbTFSRU9YeTRVYW5EZXhVTCtYalBPMU5kQ0pQclY2UmowenRISGdXWFJKc2pBRDU5cFd0MWFIM0h0R2JMMG1pdmFFcUUyZEZiZTVnRHlqREc4NFpyenBObnRkR1d1VDZUbTF6V0xoeUU0eW1hS09hM1Z0OTFMMk90RVk5S0tpRVRvN3lxb2dKenNjPS0tRTFGQjRkUDlIcERqZHhzS0VMOStHZz09--c64448e8a7bb4d5ee3889e7c47cb512093accc1b"
}

response = requests.get(url, headers=headers)

data = response.content

with open("tuicool.html", "wb") as f:
    f.write(data)
```

#### 2.5.4.2 session的使用

```python
# 直接使用Session
import requests

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

urls = ['https://www.baidu.com/s?&wd=%E8%83%8C%E6%99%AF', 'https://www.baidu.com/s?&wd=%E8%83%8C%E6%99%AF']

session = requests.Session()
with session:
    for url in urls:
        response = session.get(url, headers={'User-Agent': ua})

        with response:
            print(type(response))
            print(response.url)
            print(response.status_code)
            print(response.request.headers)  # 请求头
            print(response.cookies)          # 响应的cookie
            print(response.text[:20])        # HTML内容
```

### 2.5.5 post请求

get是从服务器上获取数据；post是向服务器传送数据。

get是把参数数据队列加到提交表单的ACTION属性所指的URL中，值和表单内各个字段一一对应，在URL中可以看到。post是通过HTTP post机制，将表单内各个字段与其内容放置在HTMLHEADER内一起传送到ACTION属性所指的URL地址，用户看不到这个过程。

对于get方式，服务器端用Request.QueryString获取变量的值，对于post方式，服务器端用Request.Form获取提交的数据。

get传送的数据量较小，不能大于2KB；post传送的数据量较大，一般被默认为不受限制。

get安全性非常低；post安全性较高。

**使用形式：**`requests.post(url, headers=headers, data=data)`

**实例：**

```python
import requests

url = "https://www.tuicool.com/login"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "cookie": "UM_distinctid=17ae2b71b4e19-0c179840ccffbb-35637203-13c680-17ae2b71b4fac6; _tuicool_session=T2NSSVFZbE9TejhYNGpwVWl3TkJhaDJNdXhYVnJ6N1RiR0t5T0UvRjlGZWtPVkJPay92bHM2L2VIbGRzZnpFLzVDM203MnRVa3VaQlRpU1o0WEtjQUVtWnJ2ZjkyeVRHOUgrMEhCMW9rZWwrZm9lL1ZMaXNVNmlQOTdFeTVUdlU4QjRoazZJdDk0N3lKQlM3N2laYmtvZUhacFp4V3BaclZEcGJBV1pPOFpBT1lnZTlUSzN1TFZVZ1RkOG5lOC84aHp4YjRrYzJGbDJRWC9yQkh6VXFpS1d1bE1YSzc5a044elpLYnJrMStBRkNlUlU2Y28xcDNXV0NRSll4T3RlMUtHbVg0aXl3U2lXMFlCS0pVK0NET3poME1uajZqN1NZMS9tUWJVMHBja2pmZlBEa0FPdGkwTnBMeDUzZVZ1NUtLTHRsUUtwOE5ldlpkcXdIUmR5ZGFRPT0tLWZMZ3lCRDVtVmhIc2VtOFdUNlN3cVE9PQ%3D%3D--c27531b5205d44a54fec28dbe0564a89b3d073e5; CNZZDATA5541078=cnzz_eid%3D1666042269-1627301221-https%253A%252F%252Fwww.tuicool.com%252F%26ntime%3D1627306658"
}

data = {
    "authenticity_token": "+Flq98ZBtBkgHs9+SrRPmooTfRBi1AlkiqznIA/tsWWoVJbQYghiCebMslo1z8DNyemakwcNoLN9XJjnINs+0g==",
    "email": "zyyyxdz@163.com",
    "password": "Zhang890"
}


response = requests.post(url, headers=headers, data=data)

data = response.content

with open("tuicool_login.html", "wb") as f:
    f.write(data)
```

## 2.6 SSL错误

![](image/image__G59OvIvOK.png)

SSL错误就是SSL的证书不安全导致的，如果在代码中则会提示requests.exceptions.SSLError的错误信息，如何解决呢？

解决：`response = requests.get(url, verify=False)`

# 3、IP代理

为什么要使用IP代理？

为了让服务器以为爬虫不是同一个客户端在请求，防止我们的真实地址被泄露，防止被追究，防止本机IP被封。

**注意：** 不是说使用了代理IP就一定不会被反爬，即使使用了代理还可以被检测到我们是一个爬虫（一段时间内访问次数），或者会购买代理提供商把代理IP加入到反爬虫的数据库。Cookies、User-Agent等headers参数也是最基本的反爬虫检测策略。

所以我们使用代理IP的时候，使用随机的方式去选择。

代理IP也有有效期，有效期越长价格越贵，当然也有免费的代理IP（快代理等）。

## 3.1代理IP的分类

*   透明代理(TransparentProxy)

    透明代理虽然可以直接“隐藏” 你的IP地址，但是还是可以查到你是谁

*   匿名代理(AnonymousProxy)

    使用匿名代理，别人只能知道你用了代理，无法知道你是谁

*   高匿代理(Eliteproxy或HighAnonymity Proxy)

    高匿代理让别人根本无法发现你是在用代理，所以是最好的选择

## 3.2代理IP的使用

**使用形式：**

`proxies = {"http": "http://IP地址:端口号"}`

`requests.get(url, headers=headers, proxies=proxies)`

**注意：** 如果代理IP不可使用，那么就直接使用真实IP。

**实例：**

```python
import requests

url = "http://httpbin.org/get"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
}

proxies = {
    "http": "http://114.104.140.201:4216"
}

response = requests.get(url, headers=headers, proxies=proxies, timeout=3)

print(response.text)    # 代理IP不可用直接报错
```

![](image/image_Vige1qGBFM.png)

**检测代理IP：** 检测代理IP是否可用，在进行使用。

```python
import requests

proxys = ["175.146.215.172:4256", "223.156.84.167:4243", "111.79.249.188:4231", "121.226.74.13:4256", "222.78.209.188:4230"]

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
}

exists = []

def check_proxy(proxys, headers):
    url = "http://httpbin.org/get"

    for ip in proxys:
        try:
            proxies = {
                "http": "http://{}".format(ip)
            }
            requests.get(url, headers=headers, proxies=proxies, timeout=3)
        except:
            return
        exists.append(ip)


check_proxy(proxys, headers)

print(exists)
```

# 4、retrying模块

retrying是一个很好用的关于重试的Python包，可以用来自动重试一些可能会运行失败的程序段。使用retrying模块提供的retry模块，可以通过装饰器的方式使用，让被装饰的函数重复执行。

retry中可以传入参数`stop_max_attempt_number`，让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果其中有一个成功，程序则继续往后执行。

**实例：**

```python
import random
import requests
from retrying import retry

proxy = ["183.16.158.124:4267", "11.22.3.33:2222", "183.92.2.48:4234"]  # 代理IP池
rand = random.choice(proxy)    # 随机选择IP
proxies = {
    "http": "http://{}".format(rand)
}

# 用户代理
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36"
}

# 尝试3次
@retry(stop_max_attempt_number=3)
def pare_url(url):
    response = requests.get(url, headers=headers, timeout=3, proxies=proxies)
    print(response.text)
    return response


pare_url("http://httpbin.org/get")
```
