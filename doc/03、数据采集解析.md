# 1、数据采集

__同步加载：__ 同步模式，又称阻塞模式，会阻止浏览器的后续处理，停止了后续的解析，因此停止了后续的文件加载（如图像）、渲染、代码执行。

__异步加载：__ 异步加载又叫非阻塞，浏览器在下载执行 js 同时，还会继续进行后续页面的处理。

## 1.1 异步加载原因

优化脚本文件的加载提高页面的加载速度，一直是提高页面加载速度很重要的一条。因为涉及到各个浏览器对解析脚本文件的不同机制，以及加载脚本会阻塞其他资源和文件的加载，因此更多的采用异步加载。

## 1.2 异步加载案例

爬取豆瓣 [https://www.douban.com](https://www.douban.com "https://www.douban.com")，喜剧电影排行数据。

*   查看网页源码，分析数据

    通过对网页源码分析我们在 `Network—All` 中看到数据在 `XHR` 类型的数据包中。而 `XHR` 是动态类型的数据包，通过滑动鼠标会触发 js 的 ajax 请求，得到响应，会再获取一个数据包。

*   问题提出

    那么我们如何获取到动态加载类型的数据呢？

__解决：__ 

1.  首先我们要获取 XHR 动态数据包的 URL

    [https://movie.douban.com/j/chart/top\_list?type=24\&interval\_id=100%3A90\&action=\&start=0\&limit=20](https://movie.douban.com/j/chart/top_list?type=24\&interval_id=100:90\&action=\&start=0\&limit=20 "https://movie.douban.com/j/chart/top_list?type=24\&interval_id=100%3A90\&action=\&start=0\&limit=20")

    [https://movie.douban.com/j/chart/top\_list?type=24\&interval\_id=100%3A90\&action=\&start=20\&limit=20](https://movie.douban.com/j/chart/top_list?type=24\&interval_id=100:90\&action=\&start=20\&limit=20 "https://movie.douban.com/j/chart/top_list?type=24\&interval_id=100%3A90\&action=\&start=20\&limit=20")

2.  通过对 URL 地址的分析可以得出

    start — 指定获取数据的开始索引

    limit — 指定获取数据的条数

3.  解决思路

    *   方法一：通过滑动鼠标会触发 js 的 ajax 请求获取到一个个 XHR 类型的数据包，就相当于翻页，我们可以指定页数即确定 start 的大小来获取每页的数据。

    *   方法二：我们可以直接通过指定 start 和 limit 参数，获取到指定的条目。

__实例：__ 

```python
  import requests

  # 确定URL
  url = "https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action="

  params = {
      "start": 0,
      "limit": 15
  }

  headers = {
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
  }

  # 发送请求获取响应
  response = requests.get(url, headers=headers, params=params)

  # 获取数据保存数据
  with open("douban.json", "wb") as f:
      f.write(response.content)

```

__注意：__ 我们可以通过 time.sleep 来减缓爬虫的速度，以免被反爬虫。

## 1.3 数据采集总结

通过爬虫我们对音视频、HTML 页面等数据直接进行爬取、获取响应；对于异步加载数据我们也可以通过确定 XHR 动态数据包的 URL 来进行爬取。但是对于爬取到的数据如何分类呢？我们如何有效的进行数据解析？



# 2、数据解析

在爬虫爬取的数据中有很多不同类型的数据，我们需要了解数据的不同类型来有规律的提取和解析数据。

*   结构化数据

    json/xml 等

*   非结构化数据

    html 等

对于结构化数据，我们可以转化为 python 数据来进一步处理；而对于非结构化数据，我们可以通过正则表达式、xpath 等进行处理。

## 2.1 json 数据解析

JSON（JavaScript Object Notation, JS 对象标记）是一种轻量级的数据交换格式。它基于 ECMAScript（w3c 组织制定的 JS 规范）的一个子集，采用完全独立于编程语言的文本格式来存储和表示数据。

官网：[http://json.org/](http://json.org/ "http://json.org/")

### 2.1.1 json 数据类型

双引号引起来的字符串，数值，true，false，null，对象，数组，这些都是值

*   字符串
    
    由双引号包围起来的任意字符的组合，可以有转义字符。
    
*   数值
    
    有正负，有整数、浮点数。
    
*   对象
    
    无序的键值对的集合，key 必须是一个字符串，需要双引号包围这个字符串，value 可以是任意合法的值。

    格式：`{key1:value1, ..., keyn:valuen}`
    
*   数组
    
    有序的值的集合。
    
    格式：`[val1, ..., valn]`

__实例：__ 

```json
{
    "person": [
        {
            "name": "Tom",
            "age": 18
        },
        {
            "name": "Jerry",
            "age": 17
        }
    ],
    "total": 2
}
```

### 2.1.2 json 模块

json 模块支持少量 python 内建数据类型到 json 类型的转换。

__常用方法：__ 

*   `json.loads()`

    将 json 格式数据转为 python 数据类型

*   `json.dumps()`

    将 python 数据类型转为 json 格式数据

*   `json.load()`

    将 json 文件数据转为 python 数据类型

*   `json.dump()`

    将 python 数据类型转为 json 文件数据

__实例：__ 

```python
import json

p_data1 = {
    "person": [
        {
            "name": "Tom",
            "age": 18
        },
        {
            "name": "Jerry",
            "age": 17
        }
    ],
    "total": 2
}

j_data = json.dumps(p_data1)   # json字符串
print(type(j_data), j_data)    # <class 'str'> {"person": [{"name": "Tom", "age": 18}, {"name": "Jerry", "age": 17}], "total": 2}


p_data2 = json.loads(j_data)   # dict
print(type(p_data2), p_data2)  # <class 'dict'> {'person': [{'name': 'Tom', 'age': 18}, {'name': 'Jerry', 'age': 17}], 'total': 2}
```

```python
import json

p_data1 = {
    "person": [
        {
            "name": "Tom",
            "age": 18
        },
        {
            "name": "Jerry",
            "age": 17
        }
    ],
    "total": 2
}

with open("test.json", "w", encoding="utf-8") as f:
    json.dump(p_data1, f, ensure_ascii=False)
```

```python
import json

with open("test.json", "r", encoding="utf-8") as f:
    p_data = json.load(f)
    print(type(p_data), p_data)
```

__注意：__ 也可以使用第三方模块 simplejson 来对 json 数据处理，需要下载：`pip install simplejson`

## 2.2 XML 数据解析

### 2.2.1 HTML

HTML（HyperTextMark-upLanguage）即超文本标记语言，是 WWW 的描述语言。

### 2.2.2 XML

XML 即 ExtentsibleMarkup Language（可扩展标记语言），是用来定义其它语言的一种元语言，其前身是 SGML（标准通用标记语言）。它没有 tagset（标签集），也没有 grammatical rule（语法规则），但是它有 syntax rule（句法规则）。任何 XML 文档对任何类型的应用以及正确的解析都必须是 well-formed（格式良好）的，即每一个打开的标签都必须有匹配的结束标签，不得含有次序颠倒的标签，并且在语句构成上应符合技术规范的要求。XML 文档可以是有效的，但并非一定要求有效。所谓有效文档是指其符合其文档类型定义的文档。如果一个文档符合一个模式的规定，那么这个文档是模式有效的。

### 2.2.3 HTML 与 XML 的区别

通过以上对 HTML 及 XML 的了解，我们来看看他们之间到底存在着什么区别与联系

XML 和 HTML 都是用于操作数据或数据结构，在结构上大致是相同的，但它们在本质上却存在着明显的区别。

*   __语法要求不同__ 

1.  在 HTML 中不区分大小写，在 XML 中严格区分。

2.  在 HTML 中，有时不严格，如果上下文清楚地显示出段落或者列表键在何处结尾，那么你可以省略 `</p>` 或者 `</li>` 之类的结束标记。在 XML 中，是严格的树状结构，绝对不能省略掉结束标记。

3.  在 XML 中，拥有单个标记而没有匹配的结束标记的元素必须用一个 / 字符作为结尾。这样分析器就知道不用查找结束标记了。

4.  在 XML 中，属性值必须分装在引号中。在 HTML 中，引号是可用可不用的。

5.  在 HTML 中，可以拥有不带值的属性名。在 XML 中，所有的属性都必须带有相应的值。

6.  在 XML 文档中，空白部分不会被解析器自动删除；但是 HTML 是过滤掉空格的。

*   __标记不同__ 

1.  HTML 使用固有的标记；而 XML 没有固有的标记。

2.  HTML 标签是预定义的；XML 标签是免费的、自定义的、可扩展的。

*   __作用不同__ 

1.  HTML 是用来显示数据的；XML 是用来描述数据、存放数据的，所以可以作为持久化的介质！HTML 将数据和显示结合在一起，在页面中把这数据显示出来；XML 则将数据和显示分开。 XML 被设计用来描述数据，其焦点是数据的内容。HTML 被设计用来显示数据，其焦点是数据的外观。

2.  XML 不是 HTML 的替代品，XML 和 HTML 是两种不同用途的语言。 XML 不是要替换 HTML；实际上 XML 可以视作对 HTML 的补充。XML 和 HTML 的目标不同，HTML 的设计目标是显示数据并集中于数据外观，而 XML 的设计目标是描述数据并集中于数据的内容。

3.  没有任何行为的 XML。与 HTML 相似，XML 不进行任何操作。（共同点）

4.  对于 XML 最好的形容可能是：XML 是一种跨平台的，与软、硬件无关的，处理与传输信息的工具。

5.  XML 未来将会无所不在。XML 将成为最普遍的数据处理和数据传输的工具。



# 3、数据提取

## 3.1 json 数据提取

json 数据是一种存储数据的形式，json 数据对应的就是 python 中的 dict 数据类型。所以对于 json 字符串，我们可以首先转换成为 python 中的 dict 类型数据，进行数据处理。

当数据处理完成，如何有效的对数据进行提取呢？可以通过 jsonpath 模块来进行数据提取。

### 3.1.1 jsonpath 模块

__官方文档：__ [https://goessner.net/articles/JsonPath/](https://goessner.net/articles/JsonPath/ "https://goessner.net/articles/JsonPath/")

| jsonpath 语法     | 描述                         |
| ----------------- | ---------------------------- |
| \$                | 根元素                       |
| @                 | 当前元素                     |
| .                 | 子元素                       |
| ..                | 递归选取元素                 |
| \*                | 通配符，匹配所有元素         |
| \[]               | 下标运算符                   |
| \[,]              | 索引集合                     |
| \[开始:结束:步骤] | 切片运算符                   |
| ()                | 脚本表达式，使用底层脚本引擎 |
| ?()               | 应用过滤器（脚本）表达式     |

__语法：__ `jsonpath.jsonpath(data, <筛选条件>)`

__结论：__ 如果筛选到返回列表，没有筛选到返回 False

__实例：__ 

```python
import jsonpath

data = {"商店": {
    "书": [
        {"类别": "参考",
         "作者": "奈杰尔·里斯",
         "标题": "世纪语录",
         "价格": 8.95
         },
        {"category": "fiction",
         "author": "Evelyn Waugh",
         "title": "Sword of Honor",
         "price": 12.99
         },
        {"类别": "小说",
         "作者": "赫尔曼·梅尔维尔",
         "标题": "白鲸记",
         "isbn": "0-553-21311-3",
         "价格": 8.99
         },
        {"类别": "小说",
         "作者": "JRR 托尔金",
         "标题": "指环王",
         "isbn": "0-395-19395-8",
         "价格": 22.99
         }
    ],
    "自行车": {
        "颜色": "红色",
        "价格": 19.95
    }
}
}

# 获取商店的值
print(jsonpath.jsonpath(data, "$.商店"))       # [{'书': [...], '自行车': {...}}]

# 获取书的值
print(jsonpath.jsonpath(data, "$.商店.书"))    # [[{}, {}, {}]]

# 获取书的所有值
print(jsonpath.jsonpath(data, "$.商店.书.*"))  # [{}, {}, {}]

# 获取书的所有值(递归)
print(jsonpath.jsonpath(data, "$..书.*"))      # [{}, {}, {}]

# 获取第三本书的信息
print(jsonpath.jsonpath(data, "$..书[2]"))

# 获取前两本书的信息
print(jsonpath.jsonpath(data, "$..书[0,1]"))   # 包前包后
print(jsonpath.jsonpath(data, "$..书[0:2]"))   # 包前不包后

# 获取最后一本书的信息
print(jsonpath.jsonpath(data, "$..书[(@.length-1)]"))    # ()支持表达式计算

# 获取"作者的所有值"
print("111111")
print(jsonpath.jsonpath(data, "$..书[?(@.标题)]"))    # 此处中文获取不到
print(jsonpath.jsonpath(data, "$..书[?(@.isbn)]"))   # 英文就可以获取

# 获取价格大于10的所有书
print(jsonpath.jsonpath(data, "$..书[?(@.price > 10)]"))
```

**总结：** 在使用@获取当前元素的值时，取不到。而使用英文就可以。

### 3.1.2 拉勾数据提取

__目标：__ 通过对拉钩 json 数据的获取，我们想进一步提取其中的关键字城市。

__实现：__ 

```python
import json
import jsonpath
import requests

url = "http://www.lagou.com/lbs/getAllCitySearchLabels.json"

# 用户代理
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36"
}

# 发送请求获取响应
response = requests.get(url, headers=headers)

str_data = response.text  # json字符串

# 将json字符串转换成python类型
python_data = json.loads(str_data)

# 提取数据获取城市
jsonpath_data = jsonpath.jsonpath(python_data, "$..name")
print(type(jsonpath_data), jsonpath_data)

# 保存数据

# python数据类型转换成json数据
json_data = json.dumps(jsonpath_data, ensure_ascii=False)
with open("city1.json", 'w', encoding='utf-8')as f:
    f.write(json_data)

# python数据类型转换成json类型并且写入文件
with open("city2.json", 'w', encoding='utf-8')as f:
    json.dump(jsonpath_data, f, ensure_ascii=False)
```

### 3.1.3 豆瓣数据提取

__目标：__ 通过对热门电影排行的爬取，我们想进一步对爬取到的数据进行提取，提取我们想要的数据信息（电影和评分）

__实现：__ 

```python
import json
import jsonpath
import requests

url = "https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20"

headers_ = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36"
}

response = requests.get(url, headers=headers_)

str_data = response.text
python_data = json.loads(str_data)

movie_data = jsonpath.jsonpath(python_data, "$..title")
print(movie_data)

score_data = jsonpath.jsonpath(python_data, "$..score")
print(score_data)

# 电影数据整合方法1
movie_dict = {}
for i in range(len(movie_data)):
    # {"电影名":评分}
    movie_dict[movie_data[i]] = score_data[i]
print(movie_dict)

# 电影数据整合方法2
movie_dict = dict(zip(movie_data, score_data))
print(movie_dict)


# 转换为json数据并存入json文件
with open("movie.json", 'w', encoding='utf-8')as f:
    json.dump(movie_dict, f, ensure_ascii=False)
```

## 3.2 html/xml 数据提取

### 3.2.1 XPath

XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。

```xml
<?xml version="1.0" encoding="UTF-8"?>
 
<bookstore>
 
<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>
 
<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>
 
</bookstore>

```

我们需要明确什么是标签（`<book></book>`）、元素（`title`）、属性（`lang="eng"`）、文本（`Harry Potter`）、根节点等等。

#### 3.2.1.1 XPath 语法

| 表达式      | 描述                                   |
| -------- | ------------------------------------ |
| nodename | 选取此节点的所有子节点。                         |
| /        | 从根节点选取（取子节点）。                        |
| //       | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置（取子孙节点）。 |
| .        | 选取当前节点。                              |
| ..       | 选取当前节点的父节点。                          |
| @        | 选取属性。                                |
| text()   | 选取文本。                                |

__谓语（Predicates）__ 

谓语用来查找某个特定的节点或者包含某个指定的值的节点，谓语被嵌在方括号中。

| 相关函数       | 描述        |
| ---------- | --------- |
| last()     | 选取最后一个元素。 |
| position() | 选取第几个元素。  |

__选取未知节点__ 

XPath 通配符可用来选取未知的 XML 元素。

| 通配符    | 描述         |
| ------ | ---------- |
| \*     | 匹配任何元素节点。  |
| @\*    | 匹配任何属性节点。  |
| node() | 匹配任何类型的节点。 |

__选取若干路径__ 

通过在路径表达式中使用 "|" 运算符，您可以选取若干个路径。

#### 3.2.1.2 XPath 轴（Axes）

轴可定义相对于当前节点的节点集。

| 轴名称                | 结果                           |
| ------------------ | ---------------------------- |
| ancestor           | 选取当前节点的所有先辈（父、祖父等）。          |
| ancestor-or-self   | 选取当前节点的所有先辈（父、祖父等）以及当前节点本身。  |
| attribute          | 选取当前节点的所有属性。                 |
| child              | 选取当前节点的所有子元素。                |
| descendant         | 选取当前节点的所有后代元素（子、孙等）。         |
| descendant-or-self | 选取当前节点的所有后代元素（子、孙等）以及当前节点本身。 |
| following          | 选取文档中当前节点的结束标签之后的所有节点。       |
| following-sibling  | 选取当前节点之后的所有兄弟节点              |
| namespace          | 选取当前节点的所有命名空间节点。             |
| parent             | 选取当前节点的父节点。                  |
| preceding          | 选取文档中当前节点的开始标签之前的所有节点。       |
| preceding-sibling  | 选取当前节点之前的所有同级节点。             |
| self               | 选取当前节点。                      |

#### 3.2.1.3 XPath 运算符

XPath 表达式可返回节点集、字符串、逻辑值以及数字。

下面列出了可用在 XPath 表达式中的运算符：

| 运算符 | 描述      | 实例                        | 返回值                                                      |
| --- | ------- | ------------------------- | -------------------------------------------------------- |
| \|  | 计算两个节点集 | //book \| //cd            | 返回所有拥有 book 和 cd 元素的节点集                                  |
| +   | 加法      | 6 + 4                     | 10                                                       |
| -   | 减法      | 6 - 4                     | 2                                                        |
| \*  | 乘法      | 6 \* 4                    | 24                                                       |
| div | 除法      | 8 div 4                   | 2                                                        |
| =   | 等于      | price=9.80                | 如果 price 是 9.80，则返回 true。&#xA;如果 price 是 9.90，则返回 false。 |
| !=  | 不等于     | price!=9.80               | 如果 price 是 9.90，则返回 true。&#xA;如果 price 是 9.80，则返回 false。 |
| <   | 小于      | price<9.80                | 如果 price 是 9.00，则返回 true。&#xA;如果 price 是 9.90，则返回 false。 |
| <=  | 小于或等于   | price<=9.80               | 如果 price 是 9.00，则返回 true。&#xA;如果 price 是 9.90，则返回 false。 |
| >   | 大于      | price>9.80                | 如果 price 是 9.90，则返回 true。&#xA;如果 price 是 9.80，则返回 false。 |
| >=  | 大于或等于   | price>=9.80               | 如果 price 是 9.90，则返回 true。&#xA;如果 price 是 9.70，则返回 false。 |
| or  | 或       | price=9.80 or price=9.70  | 如果 price 是 9.80，则返回 true。&#xA;如果 price 是 9.50，则返回 false。 |
| and | 与       | price>9.00 and price<9.90 | 如果 price 是 9.80，则返回 true。&#xA;如果 price 是 8.50，则返回 false。 |
| mod | 计算除法的余数 | 5 mod 2                   | 1                                                        |

### 3.2.2 lxml 模块

lxml 是 python 的一个解析库，支持 HTML 和 XML 的解析，支持 XPath 解析方式，而且解析效率非常高。

我们可以导入 lxml 的 etree 库 `from lxml import etree`，利用 `etree.XML`，将字符串转化为 Element 对象，Element 对象具有 xpath 的方法，返回结果的列表，能够接受 bytes 类型的数据和 str 类型的数据。

```python
from lxml import etree
xml = etree.XML(response.text) 
ret_list = xml.xpath("xpath字符串")

from lxml import etree
html = etree.HTML(response.text) 
ret_list = html.xpath("xpath字符串")

```

__实例：__ XML 数据提取

```python
from lxml import etree

string = """
<bookstore>
 
<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>
 
<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>
 
</bookstore>
"""

xml = etree.XML(string)

# 获取根元素bookstore
print(xml.xpath("/bookstore"))             # [<Element bookstore at 0x7fa2a27f3050>]

# 获取bookstore子元素的所有book元素
print(xml.xpath("/bookstore/book"))        # [<Element book at 0x7fd43626dbe0>, <Element book at 0x7fd43626db90>]

# 获取所有book的子元素
print(xml.xpath("//book/*"))               # [<Element title at 0x7fd43626dbe0>, <Element price at 0x7fd43626db90>, <Element title at 0x7fd43626db40>, <Element price at 0x7fd43626daf0>]

# 获取属于bookstore元素后代的所有book元素
print(xml.xpath("/bookstore//book"))       # [<Element book at 0x7fd43626dbe0>, <Element book at 0x7fd43626db90>]

# 获取lang属性值为eng的所有title元素
print(xml.xpath("//title[@lang='eng']"))   # [<Element title at 0x7fe9c426cc80>, <Element title at 0x7fe9c426cc30>]

# 获取属于bookstore子元素的第一个book元素的title文本,注意下标从1开始
print(xml.xpath("/bookstore/book[1]/title/text()"))             # ['Harry Potter']

# 获取属于bookstore子元素的最后一个book元素的title文本,注意下标从1开始
print(xml.xpath("/bookstore/book[last()]/title/text()"))        # ['Learning XML']

# 获取bookstore下面的book元素的title文本，从第二个开始
print(xml.xpath("/bookstore/book[position()>1]/title/text()"))  # ['Learning XML']

# 获取所有book下的title元素，仅仅选择文本为'Harry Potter'的文本
print(xml.xpath("//book/title[text()='Harry Potter']/text()"))  # ['Harry Potter']

# 获取bookstore元素中的book元素的所有title文本，且其中price元素的值大于35
print(xml.xpath("/bookstore/book[price>35]/title/text()"))      # ['Learning XML']

# 获取bookstore元素的所有子元素
print(xml.xpath("/bookstore/*"))      # [<Element book at 0x7fc91826ccd0>, <Element book at 0x7fc91826cc80>]

# 获取所有带属性的title元素
print(xml.xpath("//title[@*]"))       # [<Element title at 0x7fb4c3a6cc80>, <Element title at 0x7fb4c3a6cc30>]

# 选取book元素的所有title元素和price元素
print(xml.xpath("//book/title | //book/price"))

```

__实例：__ HTML 数据提取

```python
from lxml import etree

data = """
<div><ul>
    <li class="item-1"><a href="link1.html">first item</a></li>
    <li class="item-1"><a href="link2.html">second item</a></li>
    <li class="item-inactive"><a href="link3.html">shird item</a></li>
    <li class="item-1"><a href="link4.html">fourth item</a></li>
    <li class="item-0"><a href="link5.html">fifth item</a></li>
</ul></div>
"""

html = etree.HTML(data)
print(type(html), html)

bytesData = etree.tostring(html)  # 获取bytes类型
print(type(bytesData), bytesData)

strData = bytesData.decode()      # bytes类型解码
print(type(strData), strData)
```

__注意：__ lxml 可以把缺失的 HTML 标签补全

```python
from lxml import etree

data = """
<div><ul>
    <li class="item-1"><a href="link1.html">first item</a></li>
    <li class="item-1"><a href="link2.html">second item</a></li>
    <li class="item-inactive"><a href="link3.html">shird item</a></li>
    <li class="item-1"><a href="link4.html">fourth item</a></li>
    <li class="item-0"><a href="link5.html">fifth item</a></li>
</ul></div>
"""

html = etree.HTML(data)
print(html.xpath("//li[@class='item-1']/a/@href"), html.xpath("//li[@class='item-1']//text()"))
target = dict(zip(html.xpath("//li[@class='item-1']/a/@href"), html.xpath("//li[@class='item-1']//text()")))
print(target)



# 执行结果
['link1.html', 'link2.html', 'link4.html'] ['first item', 'second item', 'fourth item']
{'link1.html': 'first item', 'link2.html': 'second item', 'link4.html': 'fourth item'}

```

__假设：__ herf 属性缺失呢？

```python
from lxml import etree

data = """
<div><ul>
    <li class="item-1"><a>first item</a></li>
    <li class="item-1"><a href="link2.html">second item</a></li>
    <li class="item-inactive"><a href="link3.html">shird item</a></li>
    <li class="item-1"><a href="link4.html">fourth item</a></li>
    <li class="item-0"><a href="link5.html">fifth item</a></li>
</ul></div>
"""

html = etree.HTML(data)
li_element = html.xpath("//li[@class='item-1']")
print(type(li_element), li_element)
# target = {}

for li in li_element:
    item = {}
    item["href"] = li.xpath("./a/@href")[0] if len(li.xpath("./a/@href")) else None
    item["title"] = li.xpath("./a/text()")[0] if len(li.xpath("./a/text()")) else None
    print(item)



# 执行结果
<class 'list'> [<Element li at 0x7fc83fa6bf50>, <Element li at 0x7fc83fa6bf00>, <Element li at 0x7fc83fa6beb0>]
{'href': None, 'title': 'first item'}
{'href': 'link2.html', 'title': 'second item'}
{'href': 'link4.html', 'title': 'fourth item'} 
```

### 3.2.3 实例

#### 3.2.3.1 豆瓣电影 top250 爬取

__URL：__ [https://movie.douban.com/top250](https://movie.douban.com/top250 "https://movie.douban.com/top250")

<img width="900" alt="截屏2022-05-27 16 43 52" src="https://user-images.githubusercontent.com/68216337/170665856-80adddf6-7fe2-4568-9b7d-fd027cefe9ad.png">

__流程分析：__ 在 Chrome 中的 XPath 插件中调试

1.  选择所有的 span 下的文本（电影名）
2.  获取所有的 span 标签的 href（电影的 url）
3.  拼接文本：`{电影名：url}`

__实例：__ 

```python
import requests
from lxml import etree
import json

url = "https://movie.douban.com/top250"

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
}

response = requests.get(url, headers=headers)
html = etree.HTML(response.text)
movie = html.xpath("//span[@class='title'][1]/text()")    # xpath工具中不写text()也会获取文本，但是代码中要写
movie_url = html.xpath("//div[@class='hd']/a[@class='']/@href")
target = dict(zip(movie, movie_url))
print(target)

with open("douban_top250.json", "w", encoding="utf-8") as f:
    json.dump(target, f, ensure_ascii=False)
```

__翻页爬取：__ 

```python
import requests
from lxml import etree

pages = int(input("请输入需要获取的页数: "))

for i in range(pages):
    p = i * 25
    url = "https://movie.douban.com/top250?start={}&filter=".format(p)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
    }

    response = requests.get(url, headers=headers)
    html = etree.HTML(response.text)
    movie = html.xpath("//span[@class='title'][1]/text()")
    movie_url = html.xpath("//div[@class='hd']/a[@class='']/@href")
    target = dict(zip(movie, movie_url))
    print(target)
```

#### 3.2.3.2 腾讯课堂评论爬取

__流程分析：__ 

1.  分析页面确定 URL

    异步加载（XHR 动态类型数据）

2.  获取数据，保存数据

```python
import requests
import jsonpath

url = "https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page=0&filter_rating=0&bkn=1508709708&r=0.4223521937388812"

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'cookie': 'pgv_pvid=1137038448; fqm_pvqid=3d6ccae7-dc42-4c2d-a784-b1f2c5d4bf2f; tvfe_boss_uuid=7ca361dd0ee31f7c; pgv_info=ssid=s3075350362; _pathcode=0.9469249054386444; tdw_auin_data=-; iswebp=1; tdw_first_visited=1; ts_refer=www.baidu.com/link; ts_uid=1289956252; Hm_lvt_0c196c536f609d373a16d246a117fd44=1628954864; ptui_loginuin=1611748256; uin=o1611748256; skey=@KNC2sXkgL; RK=ayBIrri7YL; ptcz=acbd86ded6d4047b8f9b888ab607eb607b91135dc1ff687931fbf537397781cd; luin=o1611748256; lskey=00010000d24999efeeb4832c688b525c9a51e33438098e2641467217a210095c5f238fa5459a8095379f6094; pt4_token=oII4BsAPo2uJeLHOC2kOowHHx*TrEdBxq2z*cfmvnyk_; p_skey=0At2O1j*ZjAQOnvOcgAW2eI0aQ8*6XPgDJrvgMAMHg8_; p_lskey=000400004b6b1a78c1aa1e9f7fb1a7fd70be688c9f009b6837cf36ec0a543cbd135ff400c5a5f93c71bfa76d; auth_version=2.0; mix_login_mode=true; uid_type=0; uin=1611748256; p_uin=1611748256; p_luin=1611748256; uid_uin=1611748256; uid_origin_uid_type=0; ke_login_type=1; localInterest=[2056,2001]; index_new_key={"index_interest_cate_id":2056}; tdw_data_sessionid=162895492195118633530577; tdw_data={"ver4":"www.baidu.com","ver5":"","ver6":"","refer":"www.baidu.com","from_channel":"","path":"dh-0.9469249054386444","auin":"-","uin":1611748256,"real_uin":"611748256"}; tdw_data_testid=; tdw_data_flowid=; ts_last=ke.qq.com/course/134017; sessionPath=162895493647550137369814; Hm_lpvt_0c196c536f609d373a16d246a117fd44=1628954937; tdw_data_new_2={"auin":"-","sourcetype":"","sourcefrom":"","ver9":1611748256,"uin":1611748256,"visitor_id":"18623954480755156","sessionPath":"162895493647550137369814"}'
}

response = requests.get(url, headers=headers)
print(response.text)
```

__报错：__ `{"msg":"refer错误","type":1,"retcode":100101}`

__原因：__ 因为从课堂到评论经过了一次跳转，我们直接爬取评论，就直接被定义为爬虫。

__解决：__ headers 头部中加入 referer 跳转参数

```python
import requests
import jsonpath
import json

url = "https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page=0&filter_rating=0&bkn=1508709708&r=0.4223521937388812"

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'cookie': 'pgv_pvid=1137038448; fqm_pvqid=3d6ccae7-dc42-4c2d-a784-b1f2c5d4bf2f; tvfe_boss_uuid=7ca361dd0ee31f7c; pgv_info=ssid=s3075350362; _pathcode=0.9469249054386444; tdw_auin_data=-; iswebp=1; tdw_first_visited=1; ts_refer=www.baidu.com/link; ts_uid=1289956252; Hm_lvt_0c196c536f609d373a16d246a117fd44=1628954864; ptui_loginuin=1611748256; uin=o1611748256; skey=@KNC2sXkgL; RK=ayBIrri7YL; ptcz=acbd86ded6d4047b8f9b888ab607eb607b91135dc1ff687931fbf537397781cd; luin=o1611748256; lskey=00010000d24999efeeb4832c688b525c9a51e33438098e2641467217a210095c5f238fa5459a8095379f6094; pt4_token=oII4BsAPo2uJeLHOC2kOowHHx*TrEdBxq2z*cfmvnyk_; p_skey=0At2O1j*ZjAQOnvOcgAW2eI0aQ8*6XPgDJrvgMAMHg8_; p_lskey=000400004b6b1a78c1aa1e9f7fb1a7fd70be688c9f009b6837cf36ec0a543cbd135ff400c5a5f93c71bfa76d; auth_version=2.0; mix_login_mode=true; uid_type=0; uin=1611748256; p_uin=1611748256; p_luin=1611748256; uid_uin=1611748256; uid_origin_uid_type=0; ke_login_type=1; localInterest=[2056,2001]; index_new_key={"index_interest_cate_id":2056}; tdw_data_sessionid=162895492195118633530577; tdw_data={"ver4":"www.baidu.com","ver5":"","ver6":"","refer":"www.baidu.com","from_channel":"","path":"dh-0.9469249054386444","auin":"-","uin":1611748256,"real_uin":"611748256"}; tdw_data_testid=; tdw_data_flowid=; ts_last=ke.qq.com/course/134017; sessionPath=162895493647550137369814; Hm_lpvt_0c196c536f609d373a16d246a117fd44=1628954937; tdw_data_new_2={"auin":"-","sourcetype":"","sourcefrom":"","ver9":1611748256,"uin":1611748256,"visitor_id":"18623954480755156","sessionPath":"162895493647550137369814"}',
    'referer': 'https://ke.qq.com/course/list/python%E9%A9%AC%E5%93%A5'
}

response = requests.get(url, headers=headers)

# json——>dict方式一：
# data = json.loads(response.text)
# # print(data)

# json——>dict方式二：
data = response.json()
# print(type(data), data)

# 解析数据
nick_name = jsonpath.jsonpath(data, "$..nick_name")
first_comment = jsonpath.jsonpath(data, "$..first_comment")
# print(nick_name, first_comment)
target = dict(zip(nick_name, first_comment))
print(target)
```

__翻页实现：__ 

```python
import requests
import jsonpath
"""
后面的url参数是没必要的
https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page=0&filter_rating=0&bkn=1508709708&r=0.4223521937388812
https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page=1&filter_rating=0&bkn=1508709708&r=0.3035543969680856
https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page=2&filter_rating=0&bkn=1508709708&r=0.6291586902090103
"""

pages = int(input("请输入获取页面数: "))
for i in range(pages):
    url = "https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=134017&count=10&page={}&filter_rating=0&bkn=1508709708&r=0.4223521937388812".format(i)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
        'cookie': 'pgv_pvid=1137038448; fqm_pvqid=3d6ccae7-dc42-4c2d-a784-b1f2c5d4bf2f; tvfe_boss_uuid=7ca361dd0ee31f7c; pgv_info=ssid=s3075350362; _pathcode=0.9469249054386444; tdw_auin_data=-; iswebp=1; tdw_first_visited=1; ts_refer=www.baidu.com/link; ts_uid=1289956252; Hm_lvt_0c196c536f609d373a16d246a117fd44=1628954864; ptui_loginuin=1611748256; uin=o1611748256; skey=@KNC2sXkgL; RK=ayBIrri7YL; ptcz=acbd86ded6d4047b8f9b888ab607eb607b91135dc1ff687931fbf537397781cd; luin=o1611748256; lskey=00010000d24999efeeb4832c688b525c9a51e33438098e2641467217a210095c5f238fa5459a8095379f6094; pt4_token=oII4BsAPo2uJeLHOC2kOowHHx*TrEdBxq2z*cfmvnyk_; p_skey=0At2O1j*ZjAQOnvOcgAW2eI0aQ8*6XPgDJrvgMAMHg8_; p_lskey=000400004b6b1a78c1aa1e9f7fb1a7fd70be688c9f009b6837cf36ec0a543cbd135ff400c5a5f93c71bfa76d; auth_version=2.0; mix_login_mode=true; uid_type=0; uin=1611748256; p_uin=1611748256; p_luin=1611748256; uid_uin=1611748256; uid_origin_uid_type=0; ke_login_type=1; localInterest=[2056,2001]; index_new_key={"index_interest_cate_id":2056}; tdw_data_sessionid=162895492195118633530577; tdw_data={"ver4":"www.baidu.com","ver5":"","ver6":"","refer":"www.baidu.com","from_channel":"","path":"dh-0.9469249054386444","auin":"-","uin":1611748256,"real_uin":"611748256"}; tdw_data_testid=; tdw_data_flowid=; ts_last=ke.qq.com/course/134017; sessionPath=162895493647550137369814; Hm_lpvt_0c196c536f609d373a16d246a117fd44=1628954937; tdw_data_new_2={"auin":"-","sourcetype":"","sourcefrom":"","ver9":1611748256,"uin":1611748256,"visitor_id":"18623954480755156","sessionPath":"162895493647550137369814"}',
        'referer': 'https://ke.qq.com/course/list/python%E9%A9%AC%E5%93%A5'
    }

    response = requests.get(url, headers=headers)

    data = response.json()

    nick_name = jsonpath.jsonpath(data, "$..nick_name")
    first_comment = jsonpath.jsonpath(data, "$..first_comment")
    target = dict(zip(nick_name, first_comment))
    print(target)
```

#### 3.2.3.3 腾讯视频评论入库

```python
import requests
import jsonpath
import pymysql

url = "https://ke.qq.com/cgi-bin/comment_new/course_comment_list?cid=101461&count=10&page=1&filter_rating=0&bkn=1508709708&r=0.008691173978974343"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "referer": "https://ke.qq.com/course/101461?taid=1311382364589141"
}

response = requests.get(url, headers=headers)
data = response.json()
# print(type(data), data)    # dict
nick_name = jsonpath.jsonpath(data, "$..nick_name")
first_comment = jsonpath.jsonpath(data, "$..first_comment")
target = dict(zip(nick_name, first_comment))

db = pymysql.connect(host='localhost', user='root', password='Zhang890', database='crawler')
cursor = db.cursor()
for k, v in target.items():
    print(k, v)
    sql = "INSERT INTO comment(name, message) VALUES ('{}', '{}')".format(k, v)
    try:
        cursor.execute(sql)
        db.commit()
    except:
        db.rollback()
db.close()
```
