# 1、概述

爬虫，应该称为网络爬虫，也叫网页蜘蛛、网络机器人、网络蚂蚁等。搜索引擎，就是网络爬虫的应用者。

为什么到了今天，反而这个词汇被频繁的提起呢？有搜索引擎不就够了吗？

实际上，大数据时代的到了，所有的企业都希望通过海量数据发现其中的价值。

所以，需要爬取对特定网站、特定类别的数据，而搜索引擎不能提供这样的功能，因此，需要自己开发爬虫来解决。

## 1.1 爬虫分类

### 1.1.1 通用爬虫

常见的就是搜索引擎（网站的自动采集、网站内容的自动更新），无差别的收集数据、存储，提取关键字，构建索引库，给用户提供搜索接口，所以爬虫只是一个开始。

__爬取一般流程：__ 

1.  初始一批 URL，将这些 URL 放到待爬取队列；

2.  从队列取出这些 URL，通过 DNS 解析 IP，对 IP 对应的站点下载 HTML 页面，保存到本地服务器中，爬取完的 URL 放到已爬取队列；

3.  分析这些网页内容，找出网页里面的其他关心的 URL 链接，继续执行第 2 步，直到爬取条件结束。

__搜索引擎如何获取一个新网站的 URL？__ 

*   新网站主动提交给搜索引擎

*   通过其它网站页面中设置的外链

*   搜索引擎和 DNS 服务商合作，获取最新收录的网站

### 1.1.2 聚焦爬虫

有针对的编写特定领域数据（体育板块、娱乐板块）的爬取程序，针对某些类别数据采集的爬虫，是面向主题的爬虫。

## 1.2 Robots 协议

指定一个 robot.txt 文件，告诉爬虫引擎什么可以爬取，实例（淘宝 robot.txt）：

```
User-agent: Baiduspider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /product/
Disallow: /
允许Baiduspider爬寻article、oshtml、wenzhang整个目录，禁止爬寻product目录下面的目录。其它禁止访问。

User-Agent: Googlebot
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Googlebot爬寻article、oshtml、product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-agent: Bingbot
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Bingbot爬寻article、oshtml、product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-Agent: 360Spider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /
允许360Spider爬寻article、oshtml、wenzhang整个目录。其它禁止访问。

User-Agent: Yisouspider
Allow: /article
Allow: /oshtml
Allow: /wenzhang
Disallow: /
允许Yisouspider 爬寻article、oshtml、wenzhang整个目录。其它禁止访问。

User-Agent: Sogouspider
Allow: /article
Allow: /oshtml
Allow: /product
Allow: /wenzhang
Disallow: /
允许Sogouspider爬寻article、oshtml、product、wenzhang整个目录。其它禁止访问。

User-Agent: Yahoo! Slurp
Allow: /product
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
允许Yahoo! Slurp爬寻product、spu、dianpu、wenzhang、oversea整个目录。其它禁止访问。

User-Agent: *
Disallow: /
禁止所有搜索引擎访问网站的任何部分
```

- 其它爬虫，不允许爬取
- 这是一个君子协定，“爬亦有道”

这个协议为了让搜素引擎更有效率搜索自己内容，提供了如 Sitemap 这样的文件。

这个文件禁止爬取的往往又是可能我们感兴趣的内容，它反而泄露了这些地址。

## 1.3 网页源码分析

<img width="900" alt="截屏2022-05-26 15 11 31" src="https://user-images.githubusercontent.com/68216337/170437605-e2130335-d11a-48bc-82f0-9c9bab018392.png">



# 2、 HTTP 请求和响应处理

其实爬取网页就是通过 HTTP 协议访问网页，不过通过浏览器访问往往是人的行为，把这种行为变成使用程序来访问。

## 2.1 urllib 包

urllib 是标准库，它是一个工具包模块，包含下面模块来处理 url：

*   urllib.request 用于打开和读写 url

*   urllib.error 包含了由 urllib.request 引起的异常

*   urllib.parse 用于解析 url

*   urllib.robotparser 分析 robots.txt 文件

Python2 中提供了 urllib 和 urllib2。urllib 提供较为底层的接口，urllib2 对 urllib 进行了进一步封装。Python3 中将 urllib 合并到了 urllib2 中，并只提供了标准库 urllib 包。

### 2.1.1 urllib.request 模块

urllib.request 模块定义了在基本和摘要式身份验证、重定向、cookies 等应用中打开 url（主要是 HTTP）的函数和类。

#### 2.1.1.1 urlopen 方法

```python
# 语法
urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):

# 参数:
  url    : 链接地址字符串或request对象
  data   : 提交的数据，如果为None发起GET请求，否则发起POST请求。返回http.client.HTTPResponse类的响应对象这是一个类文件对象。
  timeout: 超时时间
  ca*    : 证书
```

__实例：__ 

```python
from urllib.request import urlopen

url = 'http://www.bing.com'

# get method
response = urlopen(url, timeout=5)  # Open url link, return response object(file-like object)
print(response.closed)              # False

with response:
    print(f'type: {type(response)}')         # type: <class 'http.client.HTTPResponse'>
    print(f'status: {response.status}')      # status: 200
    print(f'reason: {response.reason}')      # reason: OK
    print(f'real url: {response.geturl()}')  # real url: http://cn.bing.com/
    print(f'headers: {response.info()}')     # headers
    print(f'read: {response.read()}')        # 读取返回内容
    print(f'method: {response._method}')     # method: GET

print(response.closed)              # True
```

上例，通过 urllib.request.urlopen 方法，发起一个 HTTP 的 GET 请求，WEB 服务器返回了网页内容。响应的数据被封装到类文件对象中，可以通过 read 方法、readline 方法、readlines 方法获取数据，status 和 reason 属性表示返回的状态码，info 方法返回头信息，等等。

__User-agent 问题：__ 

上例的代码非常精简，即可以获得网站的响应数据。urlopen 方法只能传递 url 和 data 这样的数据，不能构造 HTTP 的请求。例如 useragent。

源码中构造的 useragent 如下：

```python
from urllib.request import OpenerDirector    # Ctrl+点击: OpenerDirector，查看源码
class OpenerDirector:
    def __init__(self):
        client_version = "Python-urllib/%s" % __version__
        self.addheaders = [('User-agent', client_version)]
```

当前显示为：Python-urllib/3.7

有些网站是反爬虫的，所以要把爬虫伪装成浏览器。随便打开一个浏览器，复制浏览器的 UA 值用来伪装。

__User-agent 大全：__ 

```python
'''
# Opera
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60
Opera/8.0 (Windows NT 5.1; U; en)
Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50
 
# Firefox
Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0
Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10
 
# Safari
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2
 
# chrome
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
 
# 360
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36
Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko
 
# 淘宝浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11
 
# 猎豹浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER) 
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)"

# QQ浏览器
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)

# sogou浏览器
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0
Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)

# maxthon浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36

# UC浏览器
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36
'''
```

#### 2.1.1.2 Request 方法 

__语法：__ `Request(url, data=None, headers={})`

初始化方法，构造一个请求对象。可添加一个 header 的字典。data 参数决定是 GET 还是 POST 请求。

`add_header(key, val)` 为 header 中增加一个键值对。

__实例：__ 

```python
import random
from urllib.request import urlopen, Request

# urllib.request.urlopen
url = 'http://www.bing.com'

ua_list = [
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;",
]

req = Request(url)
req.add_header('User-agent', random.choice(ua_list))  # pick one
print(f'req type: {type(req)}')                       # req type: <class 'urllib.request.Request'>

# get method
response = urlopen(req, timeout=5)           # Open url or request object, return response object(file-like object)
print(response.closed)                       # False

with response:
    print(f'type: {type(response)}')         # type: <class 'http.client.HTTPResponse'>
    print(f'status: {response.status}')      # status: 200
    print(f'reason: {response.reason}')      # reason: OK
    print(f'real url: {response.geturl()}')  # real url: http://cn.bing.com/
    print(f'headers: {response.info()}')     # headers
    print(f'read: {response.read()}')        # 读取返回内容
    print(f'method: {response._method}')     # method: GET

print(response.closed)                       # True
print(req.get_header('User-agent'))          # Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;
print('user-agent'.capitalize())             # User-agent
```

### 2.1.2 urllib.parse 模块

urllib.parse 该模块可以完成对 url 的编解码。

__实例：__ 

```python
from urllib import parse
u = parse.urlencode('http://www.bing.com')


"""
Traceback (most recent call last):
  File "/Users/zhang/datum/github/python-crawler/code/01-demo-urllib-parse.py", line 2, in <module>
    u = parse.urlencode('http://www.bing.com')
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py", line 909, in urlencode
    "or mapping object").with_traceback(tb)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py", line 901, in urlencode
    raise TypeError
TypeError: not a valid non-string sequence or mapping object
"""
```

__注意：__ urlencode 函数第一参数要求是一个字典或者二元组序列

```python
from urllib import parse

# 实例一：
u = parse.urlencode({
    "id":1,
    "name":"tom"
})
print(u)    # id=1&name=tom


# 实例二：
"""
url = "http://www.bing.com/?id=1&name=tom" ——> GET
url = "http://www.bing.com/" ——> POST
body = "id=1&name=tom"
"""
u = parse.urlencode({
    "id":1,
    "name":"tom",
    "url":"http://www.bing.com/?id=1&name=tom"
})
print(u)    # id=1&name=tom&url=http%3A%2F%2Fwww.bing.com%2F%3Fid%3D1%26name%3Dtom


# 实例三：
u = parse.urlencode({
    "id":1,
    "name":"张三",
    "url":"http://www.bing.com/?id=1&name=张三"
})
print(u)    # id=1&name=%E5%BC%A0%E4%B8%89&url=http%3A%2F%2Fwww.bing.com%2F%3Fid%3D1%26name%3D%E5%BC%A0%E4%B8%89
```

从运行结果来看冒号、斜杠、&、等号、问号等符号全部被编码了，% 之后实际上是单字节十六进制表示的值。

一般来说 url 中的地址部分，一般不需要使用中文路径，但是参数部分，不管 GET 还是 POST 方法，提交的数据中，可能有斜杆、等号、问号等符号，这样这些字符表示数据，不表示元字符。如果直接发给服务器端，就会导致接收方无法判断谁是元字符，谁是数据了。为了安全，一般会将数据部分的字符做 url 编码，这样就不会有歧义了。后来可以传送中文，同样会做编码，一般先按照字符集的 encoding 要求转换成字节序列，每一个字节对应的十六进制字符串前加上 % 即可。

```python
# 实验四：
# 网页使用utf-8编码
# https://www.baidu.com/s?wd=中
# 上面的url编码后为：https://www.baidu.com/s?wd=%E4%B8%AD

u = parse.urlencode({'wd':'中'})    # 编码 ——> wd=%E4%B8%AD
url = "https://www.baidu.com/s?{}".format(u)
print(url)    # https://www.baidu.com/s?wd=%E4%B8%AD

print('中'.encode('utf-8'))    # b'\xe4\xb8\xad'
print(parse.unquote(u))        # 解码 ——> wd=中
print(parse.unquote(url))      # https://www.baidu.com/s?wd=中
```

## 2.2 提交方法

最常用的 HTTP 交互数据的方法是 GET、POST。

GET 方法，数据是通过 URL 传递的，也就是说数据是在 HTTP 报文的 header 部分。

POST 方法，数据是放在 HTTP 报文的 body 部分提交的。

数据都是键值对形式，多个参数之间使用 & 符号连接，例如 a=1\&b=abc。

### 2.2.1 GET 方法

连接必应搜索引擎官网，获取一个搜索的 URL：https://cn.bing.com/search?q=kubernetes

需求：请写程序完成对关键字的 bing 搜索，将返回的结果保存到一个网页文件。

__实例：__ 

```python
from urllib import parse
from urllib.request import urlopen, Request
import ssl

ssl._create_default_https_context = ssl._create_unverified_context


base_url = 'http://cn.bing.com/search'
data = {'q': '云原生'}

url = '{}?{}'.format(base_url, parse.urlencode(data))

print(url)                 # http://cn.bing.com/search?q=%E4%BA%91%E5%8E%9F%E7%94%9F
print(parse.unquote(url))  # http://cn.bing.com/search?q=云原生

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

req = Request(url, headers={'User-agent': ua})

with urlopen(req) as res:
    with open('bing.html', 'wb+') as f:
        f.write(res.read())
        f.flush
```

### 2.2.2 POST 方法

测试网站：[http://httpbin.org/](http://httpbin.org/ "http://httpbin.org/")

__实例：__ 

```python
from urllib.request import Request, urlopen
from urllib.parse import urlencode

req = Request('http://httpbin.org/post')
req.add_header(
    'User-agent',
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
)

data = urlencode({'name': '张三,@=/&*', 'age': '6'})
print(data)           # name=%E5%BC%A0%E4%B8%89%2C%40%3D%2F%26%2A&age=6
print(data.encode())  # b'name=%E5%BC%A0%E4%B8%89%2C%40%3D%2F%26%2A&age=6'

# POST方法(From提交数据)不做url编码会有风险
with urlopen(req, data=data.encode()) as res:
    print(res.read())
```

### 2.2.3 处理 JSON 数据

__实例：__ 热门电影爬取

*   查看豆瓣电影：[https://movie.douban.com/](https://movie.douban.com/ "https://movie.douban.com/")，看到【最近热门电影】的热门并查看源码；

<img width="900" alt="截屏2022-05-26 17 14 17" src="https://user-images.githubusercontent.com/68216337/170458210-3384a282-c5d0-4356-a108-d786a3f4b7d5.png">

*   通过源码分析，我们知道这部分内容，是通过 AJAX 从后台拿到的 Json 数据。直接查看网页源代码并不能找到；

*   源码 — Network — XHR — Preview，右击复制链接地址；

<img width="900" alt="截屏2022-05-26 17 25 46" src="https://user-images.githubusercontent.com/68216337/170459951-d67e4adb-e330-402b-a57c-f4858b7ad801.png">

```python
# URL：
https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start=0

	type: movie(电影)
	tag: 标签，表示热门电影（%E7%83%AD%E9%97%A8）
	page_limit: 表示返回数据的总数
	page_start: 表示数据开始偏移位置
```

*   服务器返回的 Json 数据爬取

```python
from urllib.parse import urlencode
from urllib.request import Request, urlopen
import json
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"
url = 'https://movie.douban.com/j/search_subjects'

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

req = Request("{}?{}".format(url, urlencode(d)), headers={'User-agent': ua})

# 有可能失败，可以进行异常处理
with urlopen(req) as res:
    subjects = json.loads(res.read())
    print(len(subjects['subjects']))
    print(type(subjects), subjects)
```

## 2.3 HTTPS 证书忽略

HTTPS 使用 SSL 安全套接层协议，在传输层对网络数据进行加密。HTTPS 使用的时候需要证书，而证书需要 CA 认证。

CA（Certificate Authority）是数字证书认证中心的简称，是指发放、管理、废除数字证书的机构。

CA 是受信任的第三方，有 CA 签发的证书具有可信性。如果用户由于信任了 CA 签发的证书导致的损失，可以追究 CA 的法律责任。

CA 是层级结构，下级 CA 信任上级 CA，且有上级 CA 颁发给下级 CA 证书并认证。

—些网站，例如淘宝，使用 HTTPS 加密数据更加安全。

__实例：__ 

```python
from urllib.request import Request, urlopen

# req = Request('http://www.12306.cn/mormhweb/')  # 可以访问
# req = Request('https://www.baidu.com/')         # 可以访问
req = Request('https://www.12306.cn/mormhweb/')   # 报SSL认证异常

req.add_header(
    'User-agent',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'
)

with urlopen(req) as res:
    print(res._method)
    print(res.read())

"""
报错：
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)>
"""
```

通过 HTTPS 访问 12306 的时候，失败的原因在于 12306 的证书未通过 CA 认证，它是自己生成的证书，不可信。而其他网站访问，如 [https://www.baidu.com/](https://www.baidu.com/ "https://www.baidu.com/") 并没有提示的原因在于，它的证书的发行者受信任，且早就存储在当前系统中。

能否像浏览器一样，忽略证书不安全信息呢？

```python
from urllib.request import Request, urlopen
import ssl    # 导入ssl模块

# req = Request('http://www.12306.cn/mormhweb/')  # 可以访问
# req = Request('https://www.baidu.com/')         # 可以访问
req = Request('https://www.12306.cn/mormhweb/')   # 报SSL认证异常

req.add_header(
    'User-agent',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'
)

# 忽略不信任的证书: 可以忽略不校验的上下文
context = ssl._create_unverified_context()

res = urlopen(req, context=context)    # 上下文
with res:
    print(res._method)
    print(res.geturl())
    print(res.read().decode())
```

## 2.4 urllib3 库

官网网站：[https://urllib3.readthedocs.io/en/latest/](https://urllib3.readthedocs.io/en/latest/ "https://urllib3.readthedocs.io/en/latest/")

标准库 urllib 缺少了一些关键的功能，非标准的第三方库 urllib3 提供了，比如说连接池管理。

安装：`pip install urllib3`

__实例：__ 

```python
from urllib.parse import urlencode
import urllib3
from urllib3.response import HTTPResponse

url = 'https://movie.douban.com/j/search_subjects'
ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

with urllib3.PoolManager(cert_reqs='CERT_NONE') as http:  # CERT_NONE(忽略证书)
    responce = http.request('GET', '{}?{}'.format(url, urlencode(d)), headers={'User-agent': ua})
    print(type(responce))    # <class 'urllib3.response.HTTPResponse'>
    # responce: HTTPResponse = HTTPResponse()
    # responce.
    print(responce.status, responce.reason)
    print(responce.data)
    print(responce.headers)
```



## 2.5 requests 库 \*\*

requests 使用了 urllib3，但是API更加友好，推荐使用。

官方文档：[https://docs.python-requests.org/zh\_CN/latest/](https://docs.python-requests.org/zh_CN/latest/ "https://docs.python-requests.org/zh_CN/latest/")

### 2.5.1 response 常用属性

```python
from urllib.parse import urlencode

import requests

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

d = {
    'type': 'movie',
    'tag': '热门',
    'page_limit': 20,
    'page_start': 0
}

url = '{}?{}'.format('https://movie.douban.com/j/search_subjects', urlencode(d))

response = requests.request('GET', url, headers={'User-agent': ua})

with response:
    print(1, response.text)               # text
    print(2, response.content)            # bytes
    print(2, response.status_code)        # status
    print(3, response.url)                # url
    print(4, response.headers)            # 响应头
    print(5, response.request.headers)    # 响应对应的请求头
    print(6, response.cookies)            # 响应的cookie（经过了set-cookie动作）
    print(7, response.request._cookies)   # 响应对应请求的cookie
```

__注意：__ 

*   response.text 返回字符串类型，默认“iso-8859-1”编码，服务器不指定的话是根据网页的响应来猜测编码。

*   response.content 返回的是 bytes 型的二进制数据，返回字节类型。

### 2.5.2 User-agent

同样的和 urllib.request 模块一样，我们可以使用 User-agent 模拟浏览器，欺骗服务器，获取和浏览器一致的内容。

__实例：__ 

```python
import requests

url = "https://www.baidu.com/"

headers = {
    'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
}

response = requests.get(url, headers=headers)

data = response.content

with open("baidu.html", "wb") as f:
    f.write(data)
```

我们可以对比加 User-agent 用户代理之前爬取的网页数据和加了 User-agent 用户代理之后爬取的网页数据，发现加了 User-agent 之后爬取的数据明显变多；

我们可以写一个 User-agent 的列表，每次随机选出一个 User-agent 来使用（random.choice）；

fake-useragent 库，可以用来伪装请求头（User-Agent），但是不太稳定。

### 2.5.3 url 参数

在 url 地址中， 很多参数是没有用的，比如百度搜索的 url 地址，其中参数只有一个字段有用，其他的都可以删除。如何确定那些请求参数有用或者没用，可以进行参数删除尝试！

原始 url：[https://www.baidu.com/s?ie=utf-8\&f=8\&rsv\_bp=1\&rsv\_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv\_pq=dc94bf5400990e93\&rsv\_t=fc46%2BSMZiGcjWe%2FVkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i%2BQvFXstto\&rqlang=cn\&rsv\_enter=1\&rsv\_dl=tb\&rsv\_sug3=9\&rsv\_sug1=8\&rsv\_sug7=101\&rsv\_sug2=0\&rsv\_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv\_sug4=3061\&rsv\_sug=1](https://www.baidu.com/s?ie=utf-8\&f=8\&rsv_bp=1\&rsv_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv_pq=dc94bf5400990e93\&rsv_t=fc46+SMZiGcjWe/VkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i+QvFXstto\&rqlang=cn\&rsv_enter=1\&rsv_dl=tb\&rsv_sug3=9\&rsv_sug1=8\&rsv_sug7=101\&rsv_sug2=0\&rsv_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv_sug4=3061\&rsv_sug=1 "https://www.baidu.com/s?ie=utf-8\&f=8\&rsv_bp=1\&rsv_idx=1\&tn=baidu\&wd=背景\&fenlei=256\&rsv_pq=dc94bf5400990e93\&rsv_t=fc46%2BSMZiGcjWe%2FVkB98xspMhxbx3BbteGQMwmCA32rShSoJ5i%2BQvFXstto\&rqlang=cn\&rsv_enter=1\&rsv_dl=tb\&rsv_sug3=9\&rsv_sug1=8\&rsv_sug7=101\&rsv_sug2=0\&rsv_btype=i\&prefixsug=%E8%83%8C%E6%99%AF\&rsp=0\&inputT=1699\&rsv_sug4=3061\&rsv_sug=1")

简单 url：[https://www.baidu.com/s?\&wd=背景](https://www.baidu.com/s?\&wd=背景 "https://www.baidu.com/s?\&wd=背景")

__url 转义：__ urllib.parse 模块中的 quote/unquote 方法

*   quote()：将明文转为密文

*   unquote()：将密文转为明文

```python
from urllib.parse import quote, unquote
print(quote("背景"))           # %E8%83%8C%E6%99%AF
print(unquote(quote("背景")))  # 背景
```

__参数请求的形式：__ 

*   拼接 url 地址

    `url = "link?key1=value1&key2=value2"`

    `requests.get(url)`

*   字典参数

    `url = "link?"`

    `params = {key1: value1, key2: value2}`

    `requests.get(url, params=params)`

__实例：__ 发送带参数的请求

```python
import requests

url = "https://www.baidu.com/s"
val = input("请输入你要查询的页面: ")

params = {
    "wd": val
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
}

response = requests.get(url, headers=headers, params=params)
 
data = response.content

with open("{}.html".format(val), "wb") as f:
    f.write(data)
```

### 2.5.4 cookie 和 session

HTTP 协议本身是无状态的。什么是无状态呢，即服务器无法判断用户身份。Cookie 实际上是一小段的文本信息（key-value 格式）。客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用 response 向客户端浏览器颁发一个 Cookie。客户端浏览器会把 Cookie 保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该 Cookie 一同提交给服务器。服务器检查该 Cookie，以此来辨认用户状态。

Session 是另一种记录客户状态的机制，不同的是 Cookie 保存在客户端浏览器中，而 Session 保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是 Session。客户端浏览器再次访问时只需要从该 Session 中查找该客户的状态就可以了。

如果说 Cookie 机制是通过检查客户身上的“通行证”来确定客户身份的话，那么 Session 机制就是通过检查服务器上的“客户明细表”来确认客户身份。Session 相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。

#### 2.5.4.1 cookie 的使用

*   带上 cookie 的好处

    能够访问登录后的页面

    能够实现部分反反爬

*   带上 cookie 的坏处

    一套 cookie 往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫

__cookie 使用的形式：__ 

*   参数接收

    `cookies = {"Cookie": "******"}`

    `requests.get(url, headers=headers, cookies=cookies)`

*   记录在 headers 中传入

    `headers = {"User-Agent": "", "Cookie": ""}`

    `requests.get(url, headers=headers)`

__实例：__ 

```python
import requests

# 登陆状态的推酷网页
url = "https://www.tuicool.com/ah/0/"

headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"}
cookies = {"Cookie": "CNZZDATA5541078=cnzz_eid%3D1937101849-1639816777-https%253A%252F%252Fwww.tuicool.com%252F%26ntime%3D1639816777; _tuicool_session=YXVXSDRjcW9ObVJxQUhEWXBFQ1o0bFAyVDZkSWNBV1JOUWgrSitkS0dObEtSQ0VpUmFvUzZub040YkVXOXVqOHkvYmtZTk1BUEpLaklzZ1NjRXNQMjhBOXBvMEZDbEYxcHEwdXdlWlpTb2RzZG1xaWo5cjZUdTc3SVNKNFd4VTZjditPN1NFSHVlbzRSSjNmbjJjaU1yQ1dFTnRhUlRWLzZKd2VzVWU3cFB6a21PZEtwUGlmVE5nSDUveFh4UG9HSlVLOGNGelZVTG5LU3NPcnZ2RW8xWDRCTGlpK1NLazJCcHRKT0pSWDRuUzFrLzlaMDY5My9obEFRdmlMSHlSenFieHY1S3JIbkpnbTVvRG9rdkZMMWtLVVE5YzVwNkg2bzRZWXpnYnR3SlE0MFI0MzA3ZHl4eXM0bmlNZVJSUXd2U2Z2QTFKaW81ak8vcmQ5RVNMTEsvWTJPNjBzMWtWQkY2dVFQTVQzd1dnPS0tV2tNRW85ekVzSWhOZmpJeTRIQXZ0dz09--5c8528f3b3414fe1832f2c2d5db5e4e71f3ac37e"}

response = requests.get(url, headers=headers, cookies=cookies)

data = response.content

with open("tuicool.html", "wb") as f:
    f.write(data)
```

```python
import requests

# 登陆状态的推酷网页
url = "https://www.tuicool.com/a/?i=n"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "Cookie": "CNZZDATA5541078=cnzz_eid%3D1937101849-1639816777-https%253A%252F%252Fwww.tuicool.com%252F%26ntime%3D1639816777; _tuicool_session=YXVXSDRjcW9ObVJxQUhEWXBFQ1o0bFAyVDZkSWNBV1JOUWgrSitkS0dObEtSQ0VpUmFvUzZub040YkVXOXVqOHkvYmtZTk1BUEpLaklzZ1NjRXNQMjhBOXBvMEZDbEYxcHEwdXdlWlpTb2RzZG1xaWo5cjZUdTc3SVNKNFd4VTZjditPN1NFSHVlbzRSSjNmbjJjaU1yQ1dFTnRhUlRWLzZKd2VzVWU3cFB6a21PZEtwUGlmVE5nSDUveFh4UG9HSlVLOGNGelZVTG5LU3NPcnZ2RW8xWDRCTGlpK1NLazJCcHRKT0pSWDRuUzFrLzlaMDY5My9obEFRdmlMSHlSenFieHY1S3JIbkpnbTVvRG9rdkZMMWtLVVE5YzVwNkg2bzRZWXpnYnR3SlE0MFI0MzA3ZHl4eXM0bmlNZVJSUXd2U2Z2QTFKaW81ak8vcmQ5RVNMTEsvWTJPNjBzMWtWQkY2dVFQTVQzd1dnPS0tV2tNRW85ekVzSWhOZmpJeTRIQXZ0dz09--5c8528f3b3414fe1832f2c2d5db5e4e71f3ac37e"
}

response = requests.get(url, headers=headers)

data = response.content

with open("tuicool.html", "wb") as f:
    f.write(data)
```

#### 2.5.4.2 session 的使用

```python
import requests

ua = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"

urls = ['https://www.baidu.com/s?&wd=%E8%83%8C%E6%99%AF', 'https://www.baidu.com/s?&wd=%E8%83%8C%E6%99%AF']

session = requests.Session()
with session:
    for url in urls:
        response = session.get(url, headers={'User-Agent': ua})

        with response:
            print(type(response))
            print(response.url)
            print(response.status_code)
            print(response.request.headers)  # 请求头
            print(response.cookies)          # 响应的cookie
            print(response.text[:20])        # HTML内容
```

### 2.5.5 post 请求

get 是从服务器上获取数据；post 是向服务器传送数据。

get 是把参数数据队列加到提交表单的 ACTION 属性所指的 URL 中，值和表单内各个字段一一对应，在 URL 中可以看到。post 是通过 HTTP post 机制，将表单内各个字段与其内容放置在 HTMLHEADER 内一起传送到 ACTION 属性所指的 URL 地址，用户看不到这个过程。

对于 get 方式，服务器端用 Request.QueryString 获取变量的值，对于 post 方式，服务器端用 Request.Form 获取提交的数据。

get 传送的数据量较小，不能大于 2KB；post 传送的数据量较大，一般被默认为不受限制。

get 安全性非常低；post 安全性较高。

__语法：__ `requests.post(url, headers=headers, data=data)`

__实例：__ 

```python
import requests

url = "https://www.tuicool.com/login"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "cookie": "_tuicool_session=VktvMnNQbFExQ0g3ejBZZ1lHdkc5WXNtWFQ1T1U1MVJkaWVlNVdpZXpqemJYcmNXaU43U0dLVEd3LzlHRnNxUUFvV1YrYUpjaEVBakdUN0YvQzV0dzZzQTlOVUxVRStZQmVXalJ1UnRrakp6L2ZHV2p3T1g4OFpLSUxNMjlzb2JjQkFYYlF3MHBIczgxOHRHaGJETHhDeUVFRE5mcERSY2dmd3V2algxSWs0b3NBeittd0w1cEgzYVk2SW9oMDZza0h2UHFZY1BxQ3ZZelplZFRSR1dYT1pYaTNlY2E5ejgwOERjaDJCMHVxTk85NEM5UzBrVXZsdFllaDdKbk5lZVlFbGI0emRmVXpOQXBwOXk0dktyRFY5NU1XYzZkVHVqaXVlT1JKZERQdFV6bnZwcTNwWnU0QnQ2YVZSVU5ORC8rSGVpUm9RdkZlNGgvREwrcDI2Tm9BPT0tLUNFc3c4Y0E5aG1xY3Q3bHpLUDk4a3c9PQ%3D%3D--4de5cd5394d52727fa93508dec10150a1a8d84d4"
}

# 尝试登录：查看Form Data数据
data = {
    "authenticity_token": "L6tjk5jL+c48fBeG9JTr3NJ6DCzB4RLbFmLfPjbFEu9/ZNx4Nf6K4BpmtDoqbJIAqqIM62EmC2r6nXKfmAUVtA==",
    "email": "zyyyxdz@163.com",
    "password": "Zhang890"
}


response = requests.post(url, headers=headers, data=data)

data = response.content

with open("tuicool_login.html", "wb") as f:
    f.write(data)
```

## 2.6 SSL 错误

SSL 错误就是 SSL 的证书不安全导致的，如果在代码中则会提示 requests.exceptions.SSLError 的错误信息，如何解决呢？

__解决：__ `response = requests.get(url, verify=False)`

# 3、IP 代理

为什么要使用 IP 代理？

为了让服务器以为爬虫不是同一个客户端在请求，防止我们的真实地址被泄露，防止被追究，防止本机 IP 被封。

__注意：__ 不是说使用了代理 IP 就一定不会被反爬，即使使用了代理还可以被检测到我们是一个爬虫（一段时间内访问次数），或者会购买代理提供商把代理 IP 加入到反爬虫的数据库。Cookies、User-agent 等 headers 参数也是最基本的反爬虫检测策略。

所以我们使用代理 IP 的时候，使用随机的方式去选择。

代理 IP 也有有效期，有效期越长价格越贵，当然也有免费的代理 IP（快代理等）。

## 3.1 代理 IP 的分类

*   透明代理（TransparentProxy）

    透明代理虽然可以直接“隐藏” 你的 IP 地址，但是还是可以查到你是谁

*   匿名代理（AnonymousProxy）

    使用匿名代理，别人只能知道你用了代理，无法知道你是谁

*   高匿代理（Eliteproxy 或 HighAnonymity Proxy）

    高匿代理让别人根本无法发现你是在用代理，所以是最好的选择

## 3.2 代理 IP 的使用

__语法：__ 

`proxies = {"http": "http://IP地址:端口号"}`

`requests.get(url, headers=headers, proxies=proxies)`

__注意：__ 如果代理 IP 不可使用，那么就直接使用真实 IP。

__实例：__ 

```python
import requests

url = "http://httpbin.org/get"

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
}

proxies = {
    "http": "http://59.58.47.172:4213"
}

response = requests.get(url, headers=headers, proxies=proxies, timeout=3)

print(response.text)    # 代理IP不可用直接报错
```

__检测代理 IP：__ 检测代理 IP 是否可用，在进行使用。

```python
import requests

proxys = ["175.146.215.172:4256", "223.156.84.167:4243", "111.79.249.188:4231", "121.226.74.13:4256", "222.78.209.188:4230"]

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
}

exists = []

def check_proxy(proxys, headers):
    url = "http://httpbin.org/get"

    for ip in proxys:
        try:
            proxies = {
                "http": "http://{}".format(ip)
            }
            requests.get(url, headers=headers, proxies=proxies, timeout=3)
        except:
            return
        exists.append(ip)


check_proxy(proxys, headers)

print(exists)
```



# 4、retrying 模块

retrying 是一个很好用的关于重试的 Python 包，可以用来自动重试一些可能会运行失败的程序段。使用 retrying 模块提供的 retry 模块，可以通过装饰器的方式使用，让被装饰的函数重复执行。

retry 中可以传入参数 `stop_max_attempt_number`，让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果其中有一个成功，程序则继续往后执行。

__实例：__ 

```python
import random
import requests
from retrying import retry

proxy = ["183.16.158.124:4267", "11.22.3.33:2222", "183.92.2.48:4234"]  # 代理IP池
rand = random.choice(proxy)    # 随机选择IP
proxies = {
    "http": "http://{}".format(rand)
}

# 用户代理
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36"
}

# 尝试3次
@retry(stop_max_attempt_number=3)
def pare_url(url):
    response = requests.get(url, headers=headers, timeout=3, proxies=proxies)
    print(response.text)
    return response


pare_url("http://httpbin.org/get")
```
